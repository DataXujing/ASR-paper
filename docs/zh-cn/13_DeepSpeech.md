## DeepSpeech

<!-- https://blog.csdn.net/qq_30262201/article/details/102654423 -->
<!-- v1: https://zhuanlan.zhihu.com/p/38516611 -->

<!-- https://blog.csdn.net/xmdxcsj/article/details/54848838?spm=1001.2101.3001.6650.8&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-8-54848838-blog-97785158.235%5Ev27%5Epc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-8-54848838-blog-97785158.235%5Ev27%5Epc_relevant_default&utm_relevant_index=9 -->

<!-- https://blog.csdn.net/Left_Think/article/details/75577512?spm=1001.2101.3001.6650.6&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-6-75577512-blog-54848838.235%5Ev27%5Epc_relevant_default&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7EBlogCommendFromBaidu%7ERate-6-75577512-blog-54848838.235%5Ev27%5Epc_relevant_default&utm_relevant_index=10
 -->

<!-- deepspeech v2:  https://www.cnblogs.com/yanqiang/p/13371490.html -->
<!-- https://blog.ailemon.net/2019/08/20/translation-deep-speech-end-to-end-speech-recognition-in-english-and-mandarin/ -->
<!-- https://www.cnblogs.com/peter-jun/p/14415613.html -->

Deepspeech各个版本（https://github.com/PaddlePaddle/DeepSpeech）

(1) DeepSpeech V1

其中百度研究团队于2014年底发布了第一代深度语音识别系统 Deep Speech 的研究论文，系统采用了端对端的深度学习技术，也就是说，系统不需要人工设计组件对噪声、混响或扬声器波动进行建模，而是直接从语料中进行学习。采用 7000 小时的干净语音语料，通过添加人工噪音的方法生成 10 万小时的合成语音语料，并在 SWITCHBOARD评测语料上获得了 16.5% 的 WER（词错误率，是一项语音识别的通用评估标准）。

(2) DeepSpeech V2

 2015 年年底，百度 SVAIL 推出了Deep Speech 2，最初是为了改善在饭店、汽车、公共交通等嘈杂环境下英语识别的准确度问题。Deep Speech 2基于 LSTM-CTC（Connectionist Temporal Classification）端对端语音识别技术，将NLP领域的 LSTM 建模与 CTC 训练引入传统的语音识别框架里，通过深度学习网络识别嘈杂环境下的两种完全不同的语言——英语与普通话。端到端的学习能够使系统处理各种条件下的语音，包括嘈杂环境、口音及区别不同语种。在 Deep Speech 2 中，百度应用了 HPC 技术识别缩短了训练时间，使得以往在几个星期才能完成的实验只需要几天就能完成。

(3) DeepSpeech V3

2017年10月31日，百度的硅谷AI实验室发布了Deep Speech 3，进一步简化了模型，并且可以在使用预训练过的语言模型时继续进行端到端训练。

!> 目前开源版本为DeepSpeech V2

### 1.DeepSpeech V1： Deep Speech: Scaling up end-to-end speech recognition

#### 1.1 网络结构

对于传统的语音识别，通常会分为3个部分：声学模型，词典，语言模型。声学模型和语言模型都是分开进行训练的，因此这两个模型优化的损失函数不是相同的。而整个语音识别训练的目标（WER：word error rate）与这两个模型的损失函数不是一致的。

对于端到端的语音识别，模型的输入就为语音特征（输入端），而输出为识别出的文本（输出端），整个模型就只有一个神经网络的模型，而模型的损失采用的CTC Loss。这样模型就只用以一个损失函数作为训练的优化目标，不用再去优化一些无用的目标了。

DeepSpeech 1的结构如下图所示：

<div align=center>
    <img src="zh-cn/img/ch24/p1.png"   /> 
</div>

**全连接层**:
网络的前三层为全连接层，第一个全连接层的输入为语音的频谱数据(spectrograms)（注意：图中是把5帧的频谱数据当做一个 $x_t$输入到隐藏单元中，因为可能一个单词的发音对应了多个帧的频谱数据）。全连接层的输出计算公式为：

<div align=center>
    <img src="zh-cn/img/ch24/p2.png"   /> 
</div>

其中$g(.)$为隐藏单元的激活函数，本文使用了clipped Relu作为隐藏单元的激活函数，$W$为权重矩阵，$b$为偏置项，$h^{(l-1)_ t }$为第$(l-1)$层，第$t$个单元的输出。

文章中使用的clipped Relu函数表达式为：

<div align=center>
    <img src="zh-cn/img/ch24/p3.png"   /> 
</div>

**双向RNN层:**
第4层为双向RNN层，其中$h^{(f)}_ t$为前向（从左至右）的RNN层,$h^{(b)}_ t$为反向（从右至左）的RNN层，计算公式如下所示：

<div align=center>
    <img src="zh-cn/img/ch24/p4.png"   /> 
</div>

以前向RNN为例，其中$W^{(4)}h^{(3)}_ t$：代表了第三层第$t$个隐藏单元的输出与权重矩阵的乘积；$W^{(f)}_rh^{(f)}_ {t-1}$:代表了第$t-1$个前向传播RNN的输出和权重矩阵的乘积；$b^{(4)}$代表了偏置，而此处的$g(.)$为之前叙述的clipped Relu函数。

而网络的第5层则是非RNN层，主要是将第4层中的前向RNN和反向RNN求和作为隐藏单元的输出，然后经过的计算与普通的全连接层相同，其计算公式如下所示：

<div align=center>
    <img src="zh-cn/img/ch24/p6.png"   /> 
</div>

<div align=center>
    <img src="zh-cn/img/ch24/p5.png"   /> 
</div>

最后的第6层为softmax层，预测的是每个时间段内，将该段时间的语音识别为每个字符的概率。

<div align=center>
    <img src="zh-cn/img/ch24/p7.png"   /> 
</div>

模型采用的损失函数为CTC Loss,解码需要结合n-gram语言模型。


#### 1.2 训练数据

+ 训练数据进行加噪处理，使用多种短时噪音。
+ 录制语音的时候增加噪声的场景.

#### 1.3 训练优化

+ Data parallelism: 训练语料按照长度排序，然后多句并行
+ Model parallelism:按照时间切分，前半段在GPU1上面计算，负责计算RNN的forward activation；后半段在GPU2上面计算，负责计算RNN的backward activation。在中间时间点交换角色

最后的实验结果如下图所示：

<div align=center>
    <img src="zh-cn/img/ch24/p8.png"   /> 
</div>

<div align=center>
    <img src="zh-cn/img/ch24/p9.png"   /> 
</div>

------

### 2.DeepSpeech V2: Deep Speech 2: End-to-End Speech Recognition in English and Mandarin

百度的 DeepSpeech2 是语音识别业界非常知名的一个开源项目。这篇论文发表于2015年，作者人数非常多，来自于百度硅谷AI实验室语音技术组。
论文下载地址：https://arxiv.org/pdf/1512.02595.pdf （28页）
http://proceedings.mlr.press/v48/amodei16.pdf （10页，本文主要介绍10页的这个版本）
开源项目地址：https://github.com/PaddlePaddle/DeepSpeech

#### 2.0 摘要

端到端的技术的优势是，可以用深度神经网络替代传统的流水线式的手工方法，使得语音识别系统可以在噪音、口音、多语言等情况下通用。
摘要主要是讲 DeepSpeech2 采用了端到端的 ASR 技术，通过 HPC 技术等提高了训练速度，可以在几天内完成，因此可以快速迭代，从而发现更好的算法或架构。
DeepSpeech2 在一些任务上达到了 benchmark，可以与人工转录的结果相当。
此外，还可以在GPU上做批处理，从而可以较为容易地实现部署，可以服务大量用户，而且延时较低。

#### 2.1 介绍

数十年的手工工程领域知识已经进入当前最先进的自动语音识别（ASR）流程。一个简单但功能强大的替代解决方案是端到端地训练这样的ASR模型，使用深度学习的单个模型替换大多数模块。在这种基于端到端深度学习的系统上，我们可以采用一系列深度学习技术：收集大量训练集，使用高性能计算训练大型模型，以及有条理地探索神经网络架构的空间。

本文详细介绍了我们对模型体系结构，大型标记训练数据集以及语音识别计算规模的贡献。这包括对模型架构的广泛探索，以及我们的数据收集流程，使我们能够创建比通常用于训练语音识别系统的数据集更大的数据集。

我们在几个公开可用的测试集上对我们的系统进行基准测试，目标是最终获得人类级别的性能，不仅可以通过特定的基准测试，还可以通过对特定数据集的调整来提高，而且在一系列那些人类擅长的场景的基准测试中反映出多样化的基准测试。为此，我们还测量了每个基准测试中人工的表现以进行比较。我们发现，我们最好的中文普通话语音系统比典型的中文普通话发音者更能像短语一样转录语音查询。

本文的其余部分如下。我们首先回顾第2节中深度学习，端到端语音识别和可扩展性方面的相关工作。第3节描述了模型的架构和算法改进，第4节解释了如何有效地计算它们。我们将讨论训练数据以及为进一步增强第5节中的训练集所采取的步骤。第6节介绍了我们的系统在英语和中文普通话上的结果分析。最后第7节中介绍了将系统部署给真实用户所需的步骤。


#### 2.2 相关工作

这项工作的灵感来自先前在深度学习和语音识别方面的工作。20多年前Bourlard＆Morgan（1993）探索了前馈神经网络声学模型。循环神经网络和具有卷积的网络也被同时用于语音识别。最近，DNN已成为ASR流程中的一个固定组成，几乎所有最先进的语音工作都包含某种形式的深度神经网络。人们还发现，卷积网络对于声学模型是有益的。后来循环神经网络开始被部署在最先进的识别器上，并且与卷积层一起用于特征提取。

端到端语音识别是一个活跃的研究领域，当用于重新评估DNN-HMM的输出时，显示出令人信服的结果。RNN编码器 – 解码器范式使用编码器RNN将输入映射到固定长度矢量，解码器网络则将固定长度矢量映射为输出预测序列。有着注意力的RNN编码器 – 解码器在预测音素方面表现良好。CTC损失函数与RNN相结合来模拟时间信息，在具有字符输出的端到端语音识别中也表现良好。CTC-RNN模型也可以很好地预测音素，虽然在这种情况下仍需要词典。

迄今为止，在深度学习中利用规模一直是该领域成功的核心。单个GPU上的训练取得性能大幅提升，随后将其线性地缩放到两个或更多GPU上。我们做了一些工作来提高低级深度学习原语的个人GPU效率，我们的工作建立在使用模型的并行、数据并行或两个的组合上，来创建一个快速且高度可扩展的系统，用于在语音识别中训练深度RNN。

数据也是端到端语音识别成功的关键，Hannun等人使用了超过7000小时的标记语音。数据增强在提高在计算机视觉和语音识别等深度学习的性能方面非常有效。现有的语音系统也可用于引导新的数据收集。例如，现有的语音引擎可用于对齐和过滤数千小时的有声读物。我们从这些过去的方法中汲取灵感，引导更大的数据集和数据增加，以增加我们系统的标记数据的有效数量。


#### 2.3 模型架构

下图显示了我们架构的线框图，并列出了我们在本文中详细探讨的可交换组件。我们的系统（类似于Hannun等人（2014a）的核心系统）是具有一个或多个卷积输入层的循环神经网络（RNN），其后是多个循环（单向或双向）层和一个全连接层，之后是softmax层。使用CTC损失函数对网络进行端到端的训练，它允许我们直接预测输入音频的字符序列。

网络的输入是一个指数归一化的语音片的对数频谱序列，在20ms窗口上计算。输出是每种语言的字母表。在每个输出时间步长$t$上，RNN都进行$p(l_t | x)$ 的预测，其中$l_t$是字母表中的字符或空白符号。在英语中我们有 `lt∈{a，b，c，…，z，space，撇号，空白}`，我们在其中添加了空格符号来表示单词边界。对于普通话系统，网络输出简体中文字符。

<div align=center>
    <img src="zh-cn/img/ch24/p10.png"   /> 
</div> <p align=center>我们的网络结构，其中列出了的可交换组件会在下文讨论。
我们系统的核心是，若干个CNN输入层加一个RNN，后接多个单向或双向的RNN层，及一个全连接层，一个softmax层。
我们用的是CTC损失函数，可以直接从输入语音预测字符序列。（对于双向RNN，使用ReLU，σ(x) = min{max{x,0}, 20} 作为激活函数）</p>


在推断时，CTC模型与在大规模文本上训练得到的语言模型结合。我们使用专门的beam search来最大化
$$Q(y) = log(p_{RNN}(y|x)) + αlog(p_{LM}(y)) + β_{wc}(y)$$

其中$wc(y)$是转录文本中的字符数或者单词数。$α$用于调整语言模型和CTC网络的权重。$β$允许比词表中更多的字。这些参数可以通过在开发集上用held out留出法来调。

##### 2.3.1 深度RNN网络的batch normalization（批归一化）

为了在扩大训练集时有效地利用数据，我们通过添加RNN层增加了网络的深度。但是随着网络大小和深度的增加，使用梯度下降也变得比较困难。
已经证实可以使用批归一化来更快地训练深度网络。最近的研究表明，BatchNorm可以加快RNN训练的收敛速度，尽管并不是总能改善泛化误差。
相比之下，我们发现在大数据集上使用非常深的RNN网络时，除加速训练外，我们使用BatchNorm的变种还大大改善了最终的泛化误差。

RNN的公式:

$$h_t^l = f(W^l h^{l-1}_ t + U^l h^l_ {t-1} + b)$$

其中，在时间步$t$的$l$层的激活值是通过将同一时间步$t$的前一层$h^{l-1}_ t$的激活值与在当前层的前一个时间步$t-1$的$h^l_{t-1}$的激活值相结合来计算的。

有两种方式可以将BatchNorm应用到RNN操作中。一种很自然的方式是在每次非线性操作前加上一个BatchNorm的转换$B(.)$。

$$h_t^l = f(B(W^l h^{l-1}_ t + U^l h^l_ {t-1}))$$

但这样的话，均值和方差在一个时间步的小批次上是累加的，这没有效果。

另外一种方法是按序列归一化（sequence-wise normalization），仅对垂直连接进行批量归一化。

$$h_t^l = f(B(W^l h^{l-1}_ t) + U^l h^l_ {t-1})$$

对于每个隐藏单元，我们计算整个序列中小批量中所有项的均值和方差。下图显示了这种方法收敛地很快。

<div align=center>
    <img src="zh-cn/img/ch24/p12.png"   /> 
</div> <p align=center>使用和不使用BatchNorm（BN）训练的两个模型的训练曲线。 我们看到更深的9-7网络（总共有9层，其中有7层的是vanilla双向RNN）的性能差距比较浅的5-1网络（其中5层中只有1个是双向RNN））。 我们在第一个训练时期之后开始绘制曲线，然而曲线更难以解释，由于3.2节中提到的SortaGrad curriculum方法</p>

下图显示了按顺序归一化的性能改进随网络深度的增加而增加，最深层网络的性能差异为12％。我们存储训练期间收集的神经元的均值和方差的移动平均值，并将其用于评估阶段。
<div align=center>
    <img src="zh-cn/img/ch24/p13.png"   /> 
</div>

##### 2.3.2 SortaGrad

即使使用批量标准化，我们发现CTC的训练偶尔也会不稳定，特别是在早期阶段。 为了使训练更加稳定，我们尝试了Bengio等人的训练教程（2009）; Zaremba和Sutskever（2014），它可以加速训练并实现更好的泛化。

从头开始训练非常深的网络（或具有许多步骤的RNN）可能在训练早期失败，因为输出和梯度必须通过许多调整不良的权重层传播。除了梯度爆炸，CTC经常最终将非常长的转录分配以接近零的概率，使得梯度下降非常不稳定。这种观察激发了我们标题为SortaGrad的curriculum学习策略：我们使用话语的长度作为难度的启发式，并首先训练较短（更容易）的话语。

具体而言，在第一个训练epoch，我们按照小批量中最长话语长度的递增顺序迭代训练集中的小批量。 在第一个epoch之后，训练在mini batch上恢复随机顺序。下图分别显示了具有和不具有SortaGrad的训练cost与具有7个循环层的9层模型的比较。SortaGrad提高了训练的稳定性，这种效果在没有BatchNorm的网络中尤其明显，因为它们在数值上更不稳定。

<div align=center>
    <img src="zh-cn/img/ch24/p11.png"   /> 
</div> <p align=center>随着RNN深度的变化，BatchNorm和SortaGrad的应用以及循环隐藏单元的类型，在开发集上进行的WER比较。所有网络都有38M参数-随着深度的增加，每层隐藏单元的数量减少。最后两列比较了开发集上模型的性能，因为我们更改了循环隐藏单元的类型。 </p>


##### 2.3.3 vanilla RNN和GRU的比较（vanilla就是simple的意思，朴素，简单）

目前我们使用的是简单的RNN模型，用了ReLU激活方法。不过一些更复杂的RNN变种，如LSTM，GRU在一些类似任务上也是效果很好。
我们用GRU来实验（因为在小数据集上，同样参数量的GRU和LSTM可以达到差不多的准确率，但GRU可以训练地更快且不易发散），
上图的最后两列显示，对于固定数量的参数，GRU体系结构可在所有网络深度上实现更低的WER。

##### 2.3.4 时序卷积

时序卷积通常用于语音识别，以有效地模拟可变长度话语的时间平移不变性。频率上的卷积试图通过比大型全连接网络更简洁的方式来模拟由于不同说话人引起的频谱方差。

我们尝试添加一到三层卷积。这些都在时域和频域（2D）和仅时域（1D）中。在所有情况下，我们使用“same”卷积。在某些情况下，我们在任一维度上指定一个stride（子采样），这会减小输出的大小。

我们报告了两个数据集的结果 – 一组2048个话语（“Regular Dev”）和一个噪声很大的2048个话语数据集（“Noisy Dev”），这些数据集是从CHiME 2015开发数据集Barker等人（2015年）中随机抽样的。我们发现多层1D卷积提供了非常小的好处。2D卷积基本上在噪声数据上改善了结果，同时在清洁数据上提供了小的益处。从一层1D卷积改到三层2D卷积的变化在噪声开发集上将WER提高了23.9％。

<div align=center>
    <img src="zh-cn/img/ch24/p14.png"   /> 
</div> <p align=center>卷积层的不同配置的WER比较。 在所有情况下，卷积之后是7个循环层和1个全连接层。 对于2D卷积，第一维是频率，第二维是时间。 每个模型都使用BatchNorm，SortaGrad进行训练，并具有35M参数。</p>

##### 2.3.5 lookahead卷积和单向模型

双向RNN模型很难部署到实时、低延迟的环境中，因为当用户的声音到来时，它无法实现流式处理。
仅具有前向循环的模型通常比同类的双向模型性能差，这意味着一定数量的下文对于效果提升至关重要。
一种可能的解决方案是延迟系统产生预测结果的时间，直到它具有更多上下文为止，但我们发现很难在模型中实现这种行为。

为了构建一个没有损失精度的单向模型，我们开发了一种特殊的网络层，叫做lookahead convolution。如西域所示：

<div align=center>
    <img src="zh-cn/img/ch24/p15.png"   /> 
</div> <p align=center>未来上下文大小为2的lookahead卷积架构</p>

该层学习权重以线性组合每个神经元的激活时间步长`τ`，从而使我们能够控制所需的下文的量。lookahead由参数矩阵$W∈R^{(d,τ)}$定义，其中$d$是上一层中的神经元数量。在时间步$t$对新层的激活$r_t$为:

$$r_{t,i}=\sum^{\tau+1}_ {j=1}{W_ {i,j}h_ {t+j-1,i}}, for  1 \le i \le d. \tag{5}$$

我们将lookahead卷积放在所有循环层之上。这使我们能够以更精细的粒度在lookahead卷积以下做流式计算。

##### 2.3.6 适配普通话

要将传统的语音识别流水线移植到另一种语言，通常需要大量新的基于语言的开发。
例如，人们经常需要手工设计一个声学模型。我们可能还需要明确建模基于语言的发音特征，例如普通话的音调。
由于我们的端到端系统直接预测字符，因此不再需要这些费时的工作。这使我们只需要稍微调整，就能够使用上述方法快速创建端到端的普通话语音识别系统（输出汉字）。

我们对网络所做的唯一结构更改来自于字符集。该网络输出大约6000个字符的概率，其中包括罗马字母，因为中英文混合转录很常见。
如果有字符不在表里，则会产生OOV的错误。不过这不要紧，因为我们的测试集仅有0.74％的字符是OOV的。

我们在普通话中使用字符级语言模型，因为通常文本没有做分词。
在第6.2节中，我们证明了我们的普通话语音模型在架构变化方面的改进与英文语音模型大致相同，这表明来自一种语言的建模知识可以很好地迁移给其他语言。

#### 2.4 系统优化

我们的网络具有数以千万计的参数，而一次训练实验需要几十的单精度exaFLOPs。（exa：艾，10^18, 百亿亿）
由于我们评估数据和模型假设的能力取决于训练速度，因此我们基于高性能计算（HPC）架构创建了高度优化的训练系统（集群的每个节点有8张NVIDIA Titan X，理论峰值可提供48单精度TFLOP/s）。

尽管存在许多用于在并行计算机上训练深度网络的框架，但我们发现，我们良好的扩展能力通常会被未优化的例程所困扰。因此，我们专注于精心优化用于训练的最重要例程。
具体来说，我们为OpenMPI创建了自定义的All-Reduce代码，以求和多个节点上各个GPU上的梯度，开发了针对GPU的CTC的快速实现，并使用了自定义内存分配器。
综合起来，这些技术使我们能够在每个节点上维持理论峰值性能的45％。

我们的训练使用同步SGD，将数据分配到多个GPU上并行计算，其中每个GPU复制一份模型到本地来处理当前的小批量，然后与其他所有的GPU交换计算出的梯度。
我们更喜欢同步SGD，因为它具有可重现性，这有助于发现和修复回归。但在此模式下，GPU必须在每次迭代时快速通信（使用“all-reduce”操作），以避免浪费计算周期。

以前论文的工作已经在使用异步更新来缓解此问题。相反，我们专注于优化全减少操作本身，使用减少特定工作负载的CPU-GPU通信的技术实现了4到21倍的加速。
同样，为了增强整体计算能力，我们使用了来自Nervana Systems和NVIDIA的高度优化的内核，这些内核针对我们的深度学习应用进行了调整。
我们同样发现，自定义内存分配例程对于最大程度地提高性能至关重要，因为它们减少了GPU和CPU之间的同步次数。
我们还发现，CTC成本计算占运行时间的很大一部分。由于没有开源的针对CTC进行优化的代码，因此我们开发了一种快速的GPU实施方案，将总体训练时间减少了10-20％。（后期会开源）

#### 2.5 训练数据

大型深度学习系统需要大量的带标签的训练数据。
为了训练我们的英语模型，我们使用11940个小时的标记语音，包含800万条语音，而普通话模型，使用了9400个小时的标记语音，包含1100万条。

##### 2.5.1 数据集构建

英文和普通话数据集的一部分，是从原始数据中获取的长音频片段，这些片段的转录带有噪音。为了将音频分割成几个长片段，我们将语音与转录对齐。
对于给定的音频-转录对$(x, y)$，计算最可能的alignment：
$$l^*=argmax_{l∈Align(x,y)}\prod^T_tp_{ctc}(l_t|x;\theta)$$

这是用CTC训练的RNN模型找到的Viterbi alignment。
由于CTC损失函数计算了所有的alignment，因此这并不能确定一个正确的alignment。然而我们发现当使用双向RNN时，是可以产生正确的alignment的。

为了过滤掉转录不良的剪辑，我们建立了一个简单的分类器，具有以下特征：原始CTC的cost，由序列长度标准化的CTC的cost，由转录本长度标准化的CTC的cost，序列长度与转录本长度，转录中的单词数和转录中的字符数。我们通过大量人力标注用于构建此数据集的标签。对于英语数据集，我们发现过滤流程将WER从17％降低到5％，同时保留了50％以上的样本

此外，我们通过在每个epoch添加独特噪声来动态增加数据集，其SNR在0dB和30dB之间，就像Hannun等人一样（2014A）; Sainath等人（2015年）。


##### 2.5.2 数据缩放

我们在下图中显示了增加标注训练数据量在WER上的效果。

<div align=center>
    <img src="zh-cn/img/ch24/p16.png"   /> 
</div> 

这是通过在训练之前对整个数据集进行随机抽样来完成的。
对于每个数据集，模型进行多达20轮epoch的训练，并根据在开发集留出部分（验证集）上的错误率使用早停机制，以防止过拟合。
训练集大小每增加10倍，WER就会降低约40％。
我们还发现，常规数据集和嘈杂数据集之间的WER相对差距（相对值约为60％），这意味着数据量的增多对这两种情况都有用。

#### 2.6 结果

为了更好地评估我们的语音系统在现实场景中的适用性，我们在各种测试集上进行了评估。我们使用了几个公开可用的基准以及内部收集的几个测试集。
所有模型都在第5节中所述的完整的英语数据集或完整的普通话数据集上训练了20个epoch。
我们对于每个有512条声音的小批量使用了具有Nesterov动量的随机梯度下降法。如果梯度的范数超过阈值400，则会将其重新缩放为400。
我们选择在训练阶段在验证集上表现最好的模型进行评估。
在$[1×10^{-4},10^{-4}]$范围里选择学习率，以达到最快的收敛速度，并在每个epoch之后以常数因子1.2进行退火。我们对所有模型都使用0.99的动量。

<div align=center>
    <img src="zh-cn/img/ch24/p17.png"   /> 
</div> <p align=center>我们的语音系统和人类水平表现的WER比较</p>

##### 2.6.1 英语

最好的英文模型有2层2D卷积，接着是3层单向循环层，每层有2560个GRU单元，接着是τ= 80的lookahead卷积层，用BatchNorm和SortaGrad训练。我们不会将模型适应测试集中的任何语音条件。语言模型解码参数在保留的开发集上设置一次。

我们报告了我们系统的几个测试集的结果和人类准确性的估计。我们通过要求Amazon Mechanical Turk的工作人员手动转录我们的所有测试集来获得人类水平的衡量标准。两名工作人员录制平均约5秒钟的相同音频片段。然后，我们将最好的两个转录用于最终的WER计算。大多数工人都在美国，每次转录平均花费27秒。将手工转录的结果与现有的真实值进行比较，以产生WER估计。虽然现有的真实值转录确实存在一些标签错误，但这种情况很少超过1％。这意味着真实值转写文本与人群来源转写文本之间的分歧是典型人类工作者实际准确性的良好代表。

###### 2.6.1.1 benchmark结果

具有高信噪比的阅读语音可以说是大词汇量连续语音识别中最容易的任务。我们根据华尔街日报（WSJ）阅读新闻文章的语料库和从有声读物Panayotov等人构建的LibriSpeech语料库对我们的系统进行基准测试（2015年）。下表显示我们的系统在4个测试集中的3个中优于人类。

我们还使用VoxForge<http://www.voxforge.org>数据集测试了我们的系统对常见重音的稳健性。该集包含具有许多不同口音的说话人阅读的语音。我们将这些口音分为四类：美国 – 加拿大，印度，英联邦和欧洲。我们根据VoxForge数据构建了一个测试集，每个重音组有1024个示例，共计4096个示例。除了印度口音以外，人类的表现仍然明显优于我们的系统。

最后，我们使用最近完成的第三次CHiME挑战Barker等人（2015年）的测试集，测试了我们在嘈杂语音上的表现。该数据集具有来自在真实嘈杂环境中收集的WSJ测试集的话语以及人为添加的噪声。使用CHiME音频的所有6个通道可以提供显著的性能改进。我们为所有型号使用单一通道，因为在大多数设备上访问多声道音频并不普遍。当数据来自真实的嘈杂环境而不是对干净的语音添加噪声合成时，我们的系统与人类级别性能之间的差距更大。

<div align=center>
    <img src="zh-cn/img/ch24/p17.png"   /> 
</div> <p align=center>我们的语音系统和人类水平表现的WER比较</p>

##### 2.6.2 普通话

下表中，我们比较了几个在2000个话语开发集上训练普通话语音的架构，以及1882个嘈杂语音例子的测试集。该开发集也用于调整解码参数。我们看到，使用2D卷积和BatchNorm的最深模型相对于浅RNN提升了48％。

<div align=center>
    <img src="zh-cn/img/ch24/p18.png"   /> 
</div> <p align=center>不同RNN架构的比较。开发和测试集是内部语料库。表中的所有模型每个都有大约8000万个参数。</p>

下表显示，我们最好的普通话语音识别系统相比于人工转录可以更好地转录简短的语音查询。
<div align=center>
    <img src="zh-cn/img/ch24/p19.png"   /> 
</div> <p align=center>我们在两个随机选择的测试集上对人类最佳普通话系统进行基准测试。 第一组有100个例子，由一个由5名中国人组成的委员会标记。 第二个有250个例子，并由一个人类抄写员标记。</p>

#### 2.7 模型部署

双向模型不是为实时转录而设计的：由于RNN具有多个双向层，因此转录语音要求将整个语音输入给RNN。
而且由于我们使用beam search进行解码，因此beam search成本会很高。

为了增加部署的可扩展性，同时仍提供低延迟的转录，我们构建了一个名为Batch Dispatch的批处理调度程序。
该批处理调度程序将来自用户请求的数据流组合成批进行处理，然后对这些批处理执行RNN前向传播。
使用此调度程序，我们可以在增加批处理大小从而提高效率，和增加延迟之间进行取舍。

我们使用一个贪心的批处理方案，该方案会在上一个批处理完成后立即进行下一个批处理，而不管此时准备完成多少工作。
该调度算法平衡了效率和延时，达到了相对较小的动态批次大小（每批次至多10个样本），服务器负载与批次大小的中位数线性相关。

我们在表7中看到，当系统加载10个并发流时，系统的中位数延迟为44ms，98百分位的延迟为70ms。该服务器使用一个NVIDIA Quadro K1200 GPU对RNN进行评估。
按此设计，随着服务器负载能力的提高，批处理调度程序将以更大的批进行数据处理，而延迟依然保持在很低的水平。

我们的部署系统使用半精度计算评估RNN，这对精度没有影响，但可以显着提高效率。
我们为此任务编写了自己的16位矩阵乘法例程，从而大大提高了相对较小批次的吞吐量。

使用beam search需要在n-gram语言模型中进行重复查找，其中大部分都是直接从内存而非缓存读取的。
为了降低这些查找的成本，我们采用启发式方法：仅考虑累积概率至少为p的少量字符。
在实践中，我们发现p = 0.99效果很好，此外，我们将搜索限制为40个字符。
这样的话，总体的普通话语言模型查找时间快了150倍，并且对CER的影响可以忽略不计（相对值0.1-0.3％）。

##### 2.7.1 生产环境下的深度学习语音技术

深度学习的语音方法已经集成到面向用户的SOTA的语音产品线中。
我们发现了一些关键点，这些关键点会影响我们端到端的深度学习模型的部署。

首先，即使使用了大量的通用语音数据用于训练，少量基于应用场景的训练数据也是非常宝贵的。
例如，虽然我们使用超过10,000小时的普通话语音用于训练，但是仅有500小时的基于特定应用的数据也可以显着提高实际应用的性能。
同样，基于应用的语言模型对于达到高准确率也很重要，并且我们在DeepSpeech系统中使用了强大的预训练n-gram模型。
最后，由于我们的系统是从大量带标签的训练数据中训练出来的，是直接输出字符的，而具体的应用场景会有转录的特殊要求，这些需要做后处理（例如数字格式规范化）。
因此，尽管我们的模型消除了许多复杂性，但端到端的深度学习方法仍有待进一步研究，尤其是对于其适应性及基于应用场景的研究。

#### 2.8 总结

随着数据及算力的持续增加，对于语音识别系统的提高，端到端的深度学习展现出了令人惊喜的效果。由于该方法有高度的通用性，我们可以将其应用到新的语言上。
我们构建英文及普通话这两个性能优良的语音识别系统并没有借助任何语言方面的专家知识。最后，我们还说明了如何将其部署到GPU上供很多用户请求，为用户使用端到端的深度学习技术铺平了道路。

为了达到这一目标，我们尝试了很多网络结构，发现了很多有用的技术：使用Sorta Grad、批归一化、单向模型的超前卷积来增强数值优化。
这些尝试是在一个经过优化的、高性能计算系统上实现的，该系统使我们能够在短短几天之内对大型数据集进行全量的训练。

综上所述，我们的结果证明了端到端的深度学习技术在对于多种场景下的价值。我们相信这些技术会得到更广泛的应用。

------

### 3.DeepSpeech V3：Exploring Neural Transducers for End-to-End Speech Recognition

!> 个人觉得DeepSpeech V3并没有新的idea产生，不过那是在2017年已经非常不错了！

#### 3.1 摘要

本文中，百度的工程师对比了CTC,RNN-T和attention-based Seq2Seq模型在ASR中的应用。Hub5’00 benchmark上在没有语言模型加持的情况下，Seq2Seq和RNN-T要比CTC+LM表现的更好。在百度内部数据集集中RNN-T+LM比CTC表现要好。

#### 3.2 Introduction

最近深度神经网在语音识别中取得了SOTA的结果。深度神经网络不仅可以为传统的HMM模型提供声学特征提取，还可以作为序列转换直接应用到ASR中。对于序列的transduction的最大挑战是input和output的长度不同且不固定。语音的transducer需要学习声学特征和语言输出的对齐及映射关系。本文我们对比了CTC,RNN-T,Seq2Seq with attention模型，这些模型的不同点如下：

+ 给定声学特征$x$,不同时间步的预测是条件对立的: CTC会有这样的假设，但是RNN-T和Attention model不存在
+ input和output的对齐是单调的（The alignment between input and output units is monotonic.）：CTC和RNN-T满足该条件但是Attention 模型不行，这就是传统的Transformer based的模型不能直接做流式识别的原因。
+ 硬或软对齐（Hard vs Soft alignments）：CTC和RNN-T强制处理input和output的对齐，将其作为隐变量。训练时考虑所有可能的硬对齐，而Attention model是软对齐。

CTC的模型假设时间步之间的预测是条件独立的，这显然是是存在问题的，而RNN-T和Attention可以学习一个内含的LM优化WER。

#### 3.3 Neutal Speech Transducers

一般的一个Speech transducer包含一个encoder(一般称为声学模型)，其作用是将声学特征编码为高层的抽象表示，一个decoder用来解码语言输出（输出可以是字符或词等token),input和output不是等长的，并且训练数据是非对齐的，模型需要干的事情有两件一个是对齐任务一个是token的分类任务

input sequence:

$$x=(x_1,...,x_T)$$

output sequence:
$$y=(y_1,...,y_U)$$

其中$y_u$是一个$V$维的one-hot向量。

transducer model 建模$p(y|x)$,encoder 将input $x$映射为$h=(h_1,...,h_{T^{'}})(T^{'} < T)$,encoder可以用feed-forward neural networks(DNNs),RNNs,CNNs,Transformer based encoder, Conformer based encoder, decoder定义了对齐$a$及$h$到$y$的映射。

<div align=center>
    <img src="zh-cn/img/ch24/p20.png"   /> 
</div>


##### 3.3.1 CTC

CTC计算所有可能对齐的边际分布的条件概率，并假设output predictions在时间方向上是条件独立的。在CTC中存在一个额外的"blank"标签，可以认为是no label, 引入它使得$h$和$y$是等长度的。

<div align=center>
    <img src="zh-cn/img/ch24/p21.png"   /> 
</div>

表示所有映射为最终label $y$的所有有效的对齐$a$。$P_{CTC}(y|x)$可以通过动态的前向-后向算法进行估计。需要注意的是对齐$a$是局部和单调的：

<div align=center>
    <img src="zh-cn/img/ch24/p22.png"   /> 
</div>

解码过程为了使得beam search更高效可以融入语言模型，最终解码最大化如下式子：

<div align=center>
    <img src="zh-cn/img/ch24/p23.png"   /> 
</div>

##### 3.3.2 RNN-Transducer

RNN-T和CTC类似也是建模所有对齐的边际分布的条件概率，但是和CTC不同的是RNN-T模型中加入了之前时间步的信息而不是条件独立的，对于$u$步的预测结果$y_u$不仅仅只依赖于input $h$还依赖于之前的预测$\{y_{ <u } \} $.

<div align=center>
    <img src="zh-cn/img/ch24/p24.png"   /> 
</div>

这里的$u_t$表示在时间步$t$ 消除对齐后的输出$y$的时间步。

额外的RNN（其实就是一个语言模型）用来确定对齐$a_t$，因为RNN-T需要确定预测下一个字符是否跳出该时间步：

<div align=center>
    <img src="zh-cn/img/ch24/p25.png"   /> 
</div>

<div align=center>
    <img src="zh-cn/img/ch24/p26.png"   /> 
</div>

<div align=center>
    <img src="zh-cn/img/ch24/p27.png"   /> 
</div>

##### 3.3.3 Attention Model

Attention based model 使用注意力机制对齐input和output，像RNN-T一样消除了条件独立性的假设，不同于CTC和RNN-T它没有对齐的单调性的假设是一种soft alignment:

<div align=center>
    <img src="zh-cn/img/ch24/p28.png"   /> 
</div>

<div align=center>
    <img src="zh-cn/img/ch24/p29.png"   /> 
</div>


这里的$c_u$是上下文向量(context vector),$g_u$是第$u$步的decoder的hidden states,$e_u$的计算有多重方式。本文中用到的Attention based model可以表示为：

<div align=center>
    <img src="zh-cn/img/ch24/p30.png"   /> 
</div>

attention机制使得模型的output 可以attend到任意时间步骤，这样对齐就是non-local和non-monotonic的。解码的任务最大化：

<div align=center>
    <img src="zh-cn/img/ch24/p31.png"   /> 
</div>

这里的$cov(\alpha)$是为了控制attention attend到所有的时间步骤上我不是仅仅attend到当前时间步。

!> 关于CTC, RNN-T,Attention-based model 笔者在其他章节有详细的介绍，如果深入学习这三种模型请读者移驾到相应章节学习！


#### 3.4 Performance at Scale

下面是上述三种模型在Hub5'00和百度内部DeepSpeech数据集上的测试结果

<div align=center>
    <img src="zh-cn/img/ch24/p32.png"   /> 
</div>


#### 3.5 Impact of Encoder Architecture

为了流式识别，这里Encoder测试了Forward-only的模型和Bidirectional两种模型的WER,如此下图所示：

<div align=center>
    <img src="zh-cn/img/ch24/p33.png"   /> 
</div>


#### 3.6 Alignment Visualization

下面是对三种模型的对齐的可视化：

<div align=center>
    <img src="zh-cn/img/ch24/p34.png"   /> 
</div>

+ 我们看到最左边CTC的对其在x轴上有空白跳动，那是因为CTC中"blank"插入到对齐的原因
+ 对于中间RNN-T发现相同的input（column)会attend到不同的label,这在RNN-T和Attention中很常见也满足他们模型的特性
+ CTC和RNN-T 更集中(颜色更浓)，相比于Attention而言

综上所述，本文彻底的对比了CTC,RNN-T和Attention based model,三种模型的表现大体一致，需要注意的是CTC模型训练过程简单但需要语言模型加持，RNN-T和Attention也简化了解码过程，并且要求仅在后处理阶段引入的语言模型也是有效的。因此综上RNN-T没有额外超惨解码简单，让我们相信可以成为下一代ASR模型的范式。