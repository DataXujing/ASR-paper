## RetinaNet： Focal Loss for Dense Object Detection

!> 论文地址：https://arxiv.org/abs/1708.02002 

### 0.摘要

 迄今为止最高精度的对象检测器基于由R-CNN推广的 two-stage 方法，其中分类器应用于稀疏的候选对象位置集。相比之下，在可能的物体位置的规则，密集采样上应用的 one-stage 探测器具有更快和更简单的可能性，但迄今为止已经落后于 two-stage detector的精度。在本文中，我们调查为什么会这样。发现在密集detector训练过程中遇到的极端前景 - 背景类不平衡是其中心原因。我们提出通过重塑标准交叉熵损失来解决这种类不平衡问题，从而降低分配给分类良好的例子的损失。我们的主题“Focal Loss”将训练重点放在一组稀疏的样本上，并防止大量的轻易否定因素在训练期间压倒detector。为了评估该损失的有效性，我们设计并训练了一个简单的密集detector，称之为RetinaNet。研究结果表明，当使用Focal Loss进行训练时，RetinaNet能够匹配先前 one-stage detector的速度，同时超越所有现有技术的 two-stage detector的精度。代码位于：https://github.com/facebookresearch/Detectron    

 ### 1.Interoduction 

 目前最先进的物体detector基于 two-stage ，proposal-driven 机制。正如在R-CNN框架[11]中普及的那样，第一阶段生成稀疏的候选对象位置集，第二阶段使用卷积神经网络将每个候选位置分类为前景类之一或背景。通过一系列进展[10,28,20,14]，这个两阶段框架在具有挑战性的COCO基准[21]上始终如一地达到最高精度。

 尽管two-stage detector取得了成功，但一个自然要问的问题是：一个简单的 one-stage detector是否能达到类似的精度？ one-stage detector应用于对象位置，比例和纵横比的规则，密集采样。最近关于 one-stage detector的工作，例如YOLO [26,27]和SSD [22,9]，展示了有希望的结果，相对于现有技术的 two-stage 方法，产生更快的探测器，精度在10-40％之内。

 本文进一步推动了这个问题：我们提出了一个 one-stage 物体检测器，这是第一次与更复杂的 two-stage 最先进的COCO AP相匹配，例如特征金字塔网络（FPN）[20]或 Mask R-CNN [14] Faster R-CNN [28]。为了实现这一结果，我们将训练期间的类不平衡识别视为阻碍 one-stage 检测器实现最先进精度的主要障碍，并提出消除这种障碍的新的损失函数。

 类似不平衡通过两级级联和采样启发式在类R-CNN的探测器中得到解决。proposal 阶段（例如，选择性搜索[35]，EdgeBoxes [39]，DeepMask [24,25]，RPN [28]）迅速将候选对象位置的数量缩小到较小的数量（例如，1-2k），过滤掉大多数背景样本。在第二个分类阶段，执行采样启发式算法，例如固定的前景 - 背景比（1：3）或在线难例挖掘（OHEM）[31]，以保持前景和背景之间的可管理平衡。

 相反， one-stage 检测器必须处理在图像上定期采样的更大的一组候选对象位置。在实践中，这通常相当于枚举了100k的位置，这些位置密集地覆盖了空间位置，比例和纵横比。虽然也可以应用类似的采样启发法，但是它们效率低，因为训练过程仍然由易于分类的背景示例支配。这种低效率是对象检测中的典型问题，**通常通过诸如bootstrapping[33,29]或在线难例挖掘(hard example mining)[37,8,31]等技术来解决。**

 在本文中，**我们提出了一种新的损失函数，它可以作为处理类不平衡的先前方法的一种更有效的替代方法。** 损失函数是动态缩放的交叉熵损失，其中缩放因子随着正确类中的置信度增加而衰减为零，参见图1。直观地说，这个缩放因子可以在训练期间自动降低简单示例的贡献，并快速将模型集中在较难示例上。实验表明，我们提出的Focal Loss使我们能够训练一个高精度的one-stage检测器，优于通过采样启发式或难例挖掘训练的替代方案（先前用于训练one-stage检测器的技术）。最后，我们注意到Focal Loss的确切形式并不重要，我们展示其他实例可以实现类似的结果。

<div align=center>
<img src="zh-cn/img/retinanet/p1.png" />
</div>

*图1. 我们提出了一个新的损失称为Focal Loss，它为标准交叉熵准则增加了一个$(1 - p_t)$因子。设置$\gamma >0$可以减少分类良好的例子($p_t>0.5$)的相对损失，更多地关注较难，错误分类的例子。正如我们的实验将证明的那样，所提出的Focal Loss使得能够在存在大量简单背景示例的情况下训练高度精确的密集物体检测器*

为了证明所提出的Focal Loss的有效性，我们设计了一个简单的oen-stage 检测器，称为`RetinaNet`，以其对输入图像中物体位置的密集采样而命名。它的设计具有高效的网内特征金字塔和 anchor boxes 的使用。它借鉴了[22,6,28,20]中最近的各种想法。 RetinaNet高效准确; 我们的最佳模型，`基于ResNet-101-FPN骨干网，在以5 fps运行时达到39.1的COCO测试开发AP，超过了以前最佳公布的单级和两级探测器的单模型结果`，见图2。

<div align=center>
<img src="zh-cn/img/retinanet/p2.png" />
</div>

*图2. COCO test-dev上的速度（ms）与精度（AP）的关系。由于Focal Loss，我们简单的one-stage RetinaNet检测器优于所有以前的one-stage和two-stage检测器，包括[20]Faster R-CNN [28]系统。我们展示了具有ResNet-50-FPN（蓝色圆圈）和ResNet-101-FPN（橙色钻石）的RetinaNet的五种尺度（400-800像素）的变体*

### 2.Related Work

**经典目标检测器**：滑动窗口范例，其中分类器应用于密集图像网格，具有悠久而丰富的历史。最早的成功之一是LeCun等人的经典著作，他将卷积神经网络应用于手写数字识别[19,36]。Viola和Jones [37]使用增强物体探测器进行人脸检测，从而广泛采用这种模型。HOG [4]和整体通道特征[5]的引入为行人检测提供了有效的方法。DPM [8]帮助将密集检测器扩展到更一般的对象类别，并且多年来在PASCAL [7]上取得了最佳成果。虽然滑动窗口方法是经典计算机视觉领域的主要检测范式，但随着深度学习的复兴[18]，接下来描述的两阶段探测器很快成为了对象检测的主导。(在图像上做滑窗，不同的窗口大小）。

**Two-stage检测器**：现代目标检测中的主导范式基于two-stage方法。作为选择性搜索(selective search)工作[35]的先驱，第一阶段生成一组稀疏候选提案，其中应包含所有对象，同时过滤掉大多数负面位置，第二阶段将提案分类为前景类/背景。 R-CNN [11]将第二阶段分类器升级为卷积网络，在准确性方面取得了很大的进步，并开创了现代物体检测的时代。多年来，R-CNN在速度[15,10]和使用学术对象提案[6,24,28]方面得到了改进。区域提议网络（RPN）将第二阶段分类器的提议生成集成到单个卷积网络中，形成更快的RCNN框架[28]。已经提出了对该框架的许多扩展，例如， [20,31,32,16,14]。

**One-stage 检测器**：OverFeat [30]是基于深度网络的第一个现代one-stage物体检测器之一。最近，SSD [22,9]和YOLO [26,27]对一阶段方法重新产生了兴趣。这些检测器已针对速度进行了调整，但其准确性与two-stage方法相当。 SSD的AP降低了10-20％，而YOLO专注于更加极端的速度/准确性权衡。参见图2.最近的工作表明，通过降低输入图像分辨率和提议数量，可以快速构建two-stage检测器，但即使计算预算较大，一阶段方法的准确性也会下降[17]。相比之下，**本文的目的是了解one-stage检测器是否能够以相似或更快的速度运行同时匹配或超过two-stage检测器的精度。**

RetinaNet检测器的设计与先前的密集探测器有许多相似之处，特别是RPN [28]引入的“锚点”概念以及SSD [22]和FPN [20]中特征金字塔的使用。**我们强调，我们的简单检测器不是基于网络设计的创新而是由于我们的新颖损失而取得了最佳结果。**

**Class Imbalance**：经典的one-stage目标检测方法，如增强型探测器[37,5]和DPM [8]，以及最新的方法，如SSD [22]，在训练过程中都面临着巨大的class imbalance。这些检测器评估每个图像$10^4-10^5$ 个候选位置，但只有少数位置包含对象。这种不平衡导致两个问题：

（1）训练效率低下，因为大多数位置容易产生负面影响而没有有用的学习信号; 

（2）整体而言，容易否定的东西可以压倒训练并导致退化模型。

一种常见的解决方案是执行某种形式的难例挖掘[33,37,8,31,22]，在训练期间采样难例样本或更复杂的采样/重新加权方案[2]。相比之下，Focal Loss自然地处理了one-stage 检测器所面临的类不平衡，并且允许在没有采样的情况下有效地训练所有示例，并且没有容易的负面压倒损失和计算的梯度。

**稳健估计**：人们对设计鲁棒损失函数（例如，Huber损失[13]）非常感兴趣，这些函数通过降低具有大误差的示例的丢失来减少异常值的贡献（难例）。相比之下，Focal Loss不是用于解决异常值，而是通过向下加权内部函数（简单样例）来解决类不平衡问题，这样即使它们的数量很大，它们对总损失的贡献也很小。换句话说，Focal Loss与鲁棒的损失起着相反的作用：它将训练集中在一组稀疏的难例上。

### 3.Focal Loss

Focal Loss旨在解决one-stage目标检测场景，其中在训练期间前景和背景类之间存在极端不平衡（例如，`1：1000`）。我们从二元分类的交叉熵（CE：cross entropy）损失开始引入Focal Loss：

<div align=center>
<img src="zh-cn/img/retinanet/p3.png" />
</div>

在上面的$y\in\\{-1,+1\\}$中指定了ground-truth(GT)的类别，$p\in\[0,1\]$是模型的预测label为：$y = 1$的类的估计概率。为了符号方便，我们定义了$p_t$：

<div align=center>
<img src="zh-cn/img/retinanet/p4.png" />
</div>

CE损失可以看作*图1*中的蓝色（顶部）曲线。这种损失的一个值得注意的特性，在其图中很容易看出，即使是容易分类的例子（$p_t>>0.5$）也会造成损失具有非平凡的程度。当总结大量简单的例子时，这些小的损失值可以压倒稀少的类。

#### 3.1. Balanced Cross Entropy

解决类不平衡的常用方法是引入加权因子$\alpha\in\[0,1\]$，对于类别`+1`加权$\alpha$,对于类别`-1`加权$1-\alpha$。在实践中 $\alpha$ 可以通过逆类频率设置或者作为超参数来设置以通过交叉验证来设置。为了符号方便，我们定义类似于我们如何定义$p_t$。我们将 $\alpha-balanced-CE$损失写为：

<div align=center>
<img src="zh-cn/img/retinanet/p5.png" />
</div>

这种损失是CE的简单扩展，这是我们提出的Focal Loss的实验基线。

#### 3.2. Focal Loss Definition

正如实验所表明的那样，在密集检测器训练过程中遇到的类不平衡压倒了交叉熵损失。容易分类的负样本包括大部分损失并主导梯度。虽然$\alpha$平衡`正/负`例的重要性，它没有区分`简单/困难`的样本。 相反，我们建议重塑损失函数，以减轻简单的样本，从而集中训练难例。 更正式地说，我们建议添加一个具有可调聚焦参数因子$(1-p_t)^\gamma$ ($\gamma>=0$)给交叉熵损失。我们将Focal Loss定义为：

<div align=center>
<img src="zh-cn/img/retinanet/p6.png" />
</div>

Focal Loss在图1中可视化为$\gamma\in\[0,5\]$的多个值。我们注意到Focal Loss的两个属性。

（1）当一个例子被错误分类并且$p_t$很小时，调制因子接近1并且损失不受影响。当$p_t\to1$时，因子变为0，并且分类良好的示例的损失是低权重的。 

（2）聚焦参数$\gamma$平滑地调整容易样本下降的速率。当$\gamma=0$时，FL相当于CE，并且随着$\gamma$增加，调制因子的影响同样增加（我们发现$\gamma=2$在我们的实验中效果最好）。

直观地，调制因子减少了简单示例的损耗贡献，并扩展了示例获得低损耗的范围。例如，如果$\gamma=2$，用$p_t=0.9$分类的例子 与CE相比会有100倍的更低损耗，用$p_t=0.968$会有1000倍的更低损失。这反过来又增加了纠正错误分类的例子的重要性（对于$p_t<=.5$且$\gamma=2$，其损失最多按比例缩小4倍）
在实践中，我们使用Focal Loss的$\alpha$平衡变量：

<div align=center>
<img src="zh-cn/img/retinanet/p7.png" />
</div>

在实验中采用了这种形式，因为它比非平衡形式产生略微提高的准确性。最后，我们注意到损耗层的实现将用于计算p的sigmod操作与损耗计算相结合，从而产生更大的数值稳定性。在主要实验结果中，使用上面的Focal Loss定义，其精确形式并不重要。在附录中，考虑了局部损失的其他实例，并证明这些实例同样有效。

#### 3.3. Class Imbalance and Model Initialization

默认情况下，二进制分类模型初始化为具有输出y=-1或1的相等概率。在这样的初始化下，在类不平衡的情况下，由于频繁类别引起的损失可以主导全部损失并且导致早期训练的不稳定性。为了解决这个问题，我们在训练开始时引入了模型估计的稀有类（前景）的p值的“先验”概念。我们用先前表示 $\pi$ 并设置它使得模型的稀有类的例子的估计p很低，例如0.01。我们注意到这是模型初始化的变化（见section 4.1），而不是损失函数。我们发现这可以改善在重度不平衡情况下交叉熵和局部损失的训练稳定性。

#### 3.4. Class Imbalance and Two-stage Detectors

Two-stage 检测器通常使用交叉熵损失进行训练，而不使用平衡或我们提出的loss。相反，**他们通过两种机制解决类不平衡问题：（1）两阶段级联和（2）有偏见的小批量抽样。第一个级联阶段是一个对象提议机制[35,24,28]，它将几乎无限的一组可能的对象位置减少到一两千。** 重要的是，所选择的候选框不是随机的，但可能与真实的对象位置相对应，这消除了绝大多数容易否定的问题。当训练第二阶段时，偏向采样通常用于构造微型batch，其包含例如`
1：3`比率的正反应实例。这个比率就像一个通过抽样实现的隐含的平衡因子。我们提出的Focal Loss旨在通过损失函数直接在one-stage 目标检测系统中解决这些机制。


### 4. RetinaNet Detector

RetinaNet是一个统一的网络，由骨干网（backbone network）和两个特定于任务的子网(subnetworks)组成。骨干负责计算整个输入图像上的卷积特征图，并且是一种自卷积网络。第一个子网在主干的输出上执行卷积对象分类;第二个子网执行卷积边界框回归。这两个子网具有一个简单的设计，专为one-stage目标检测，参见**图3**.虽然这些组件的细节有很多可能的选择，但大多数设计参数对于精确值并不特别敏感，如实验中所示。我们接下来描述RetinaNet的每个组件。


<div align=center>
<img src="zh-cn/img/retinanet/p8.png" />
</div>

*图3.one-stage RetinaNet网络架构在前馈ResNet架构[16]（a）之上使用特征金字塔网络（FPN）[20]骨干，以生成丰富的多尺度卷积特征金字塔（b）。对于这个骨干网，RetinaNet附加了两个子网，一个用于分类锚框（c），一个用于从anchor boxes回归到ground-truth对象框（d）。网络设计有意简单，这使得这项工作能够专注于一种新颖的Focal Loss功能，消除了我们的oen-stage 目标检测器和最先进的two-stage目标检测测器之间的精度差距，例如Faster R-CNN和FPN [ 20]以更快的速度运行。*


**FPN骨干网络**：我们采用[20]的特征金字塔网络（FPN）作为RetinaNet的骨干网络。简而言之，FPN通过自上而下的路径和横向连接增强了标准卷积网络，因此网络从单个分辨率输入图像有效地构建了丰富的多尺度特征金字塔，参见图3(a)-(b)。金字塔的每个级别可用于检测不同比例的对象。 FPN改进了完全卷积网络（FCN）的多尺度预测[23]，如其对RPN [28]和DeepMask-style proposals[24]的增益所示，以及Fast R-CNN等两级探测器[10]或Mask R-CNN [14]。

在[20]之后，我们在ResNet架构之上构建了FPN [16]。我们构建了一个级别为P3到P7的金字塔，其中$l$表示金字塔等级（$P_l$的分辨率比输入低$2^l$）。如[20]中所有金字塔等级都有C = 256个通道。金字塔的细节通常遵循[20]，只有一些适度的差异.2虽然许多设计选择并不重要，但我们强调FPN主干的使用是: 仅使用来自最终ResNet层的特征的初步实验产生了低AP。

**Anchors**: 我们使用类似于[20]中RPN变体中的平移不变锚点框。锚在金字塔等级P3至P7上分别具有322至5122的区域。如[20]所示，在每个金字塔等级，我们使用三个长宽比`1：2,  1：1,  2：1`的锚点。对于比[20]更密集的比例覆盖，在每个级别，我们添加尺寸为 $2^0,2^{1/3},2^{2/3 }$的3个长宽比锚的原始组的锚。这在我们的设置中改善了AP。总共每个级别有`A = 9`个锚点，并且它们覆盖相对于网络输入图像的比例范围`32-813`像素。

每个锚被分配一个长度为K的分类目标的one-hot  vector，其中K是对象类的数量，以及4个向量的框回归目标。我们使用来自RPN的分配规则[28]，但修改了多类检测和调整后的阈值。具体而言，使用0.5的交叉联合（IoU）阈值将锚分配给ground-truth对象框;如果他们的IoU在[0,0.4]，则为背景。由于每个锚被分配给至多一个对象框，我们将其长度K标签向量中的相应条目设置为1，将所有其他条目设置为0。如果未分配锚点（可能在[0.4,0.5]中重叠发生），则在训练期间将忽略该锚点。Box回归目标计算为每个锚点与其指定对象框之间的偏移量，如果没有赋值，则省略。

**分类子网**：分类子网预测每个A anchors和K个对象类在每个空间位置处对象存在的概率。该子网是连接到每个FPN级别的小型FCN; 所有金字塔等级共享此子网的参数。它的设计很简单。从给定的金字塔等级获取具有C通道的输入特征图，该子网应用四个`X3`卷积层，每个卷积层具有C个滤波器，每个层接着ReLU激活，接着是具有`KA`滤波器的`3X3`卷积层。最后附加sigmoid激活以输出每个空间位置的`KA`二元预测，参见图3（c）。在大多数实验中我们使用`C=256`和`A=9`。

与RPN [28]相反，我们的对象分类子网更深，仅使用`3X3`个卷积层，并且与bounding box回归子网不共享参数（下面将介绍）。我们发现这些更高级别的设计决策比超参数的特定值更重要。

**Box回归子网**：与对象分类子网并行，我们将另一个小FCN附加到每个金字塔等级，以便将每个锚框的偏移量回归到附近的ground-truth对象（如果存在）。子回归子网的设计与分类子网相同，只是它在每个空间位置终止于4A线性输出，见图3（d）。对于每个空间位置的每个A个锚点，这4个输出预测锚和ground-truth框之间的相对偏移（我们使用RCNN [11]中的标准框参数化）。我们注意到，与最近的工作不同，我们使用类不可知的边界框回归量，它使用更少的参数并且同样有效。对象分类子网和box回归子网虽然共享一个共同的结构，但使用单独的参数。

#### 4.1. Inference and Training

**Inference**：RetinaNet形成一个由ResNet-FPN骨干网，分类子网和box回归子网组成的FCN，参见图3. 因此，推理只涉及通过网络转发图像。为了提高速度，在将检测器置信度设置为0.05后，我们仅解码每FPN级别最多1k最高得分预测的box预测。合并所有级别的最高预测，并应用阈值为0.5的非最大抑制以产生最终检测。

**Focal Loss**：我们使用此工作中引入的Focal Loss作为分类子网输出的损失。正如我们将在5中展示的那样，我们发现了这一点$\gamma=2$在实践中运作良好，RetinaNet在$\gamma \in[0.5,5]$时相对鲁棒。我们强调，在训练RetinaNet时，Focal Loss应用于每个采样图像中的所有`~100k`锚点。这与使用启发式采样（RPN）或实例挖掘（OHEM，SSD）为每个minibatch选择一小组锚（例如，256）的常规做法形成对比。图像的总Focal Loss计算为所有 `~100k`锚点上的Focal Loss之和，通过分配给ground-truth框的锚点数量进行归一化。我们通过指定锚点的数量而不是总锚点来执行归一化，因为绝大多数锚点都是容易的负样本并且在Focal Loss获得可忽略的损失值。最后我们注意到，分配给稀有类的权重，也有一个稳定的范围，但它与$\gamma$相互作用，因此必须将两者一起选择（见表1a和1b）。通常，随着$\gamma$的增加，$\alpha$应略微减小（对于$\gamma=2,\alpha= 0.25$最佳）。

<div align=center>
<img src="zh-cn/img/retinanet/p9.png" />
</div>

*表1. RetinaNet和Focal Loss（FL）的消融实验。除非另有说明，否则所有model均在trainval35k上进行训练并在迷你minival上进行测试。如果未指定，则默认值为：$\gamma=2$, 3个scale和3个ratio的锚点; ResNet-50-FPN骨干;和600像素的训练和测试图像比例。 （a）具有 a-balanced CE的RetinaNet最多可达到31.1 AP。 （b）相比之下，使用具有相同精确网络的FL可获得2.9 AP增益并且相当精确$\gamma/\alpha$设置。 （c）使用2-3scale和3ratio锚点产生良好的结果，之后性能饱和。 （d）FL优于在线难例挖掘（OHEM）[31,22]的最佳变体，超过3个点AP。 （e）RetinaNet对各种网络深度和图像尺度的测试开发的准确性/速度权衡（另见图2）。*


**初始化**：我们使用ResNet-50-FPN和ResNet-101-FPN骨干进行实验[20]。基础ResNet-50和ResNet-101型号在ImageNet1k上进行了预训练;我们使用[16]发布的模型。为FPN添加的新层初始化为[20]。除了RetinaNet子网中的最后一个以外，所有新的转换层都被初始化为偏置`b=0`和高斯权重填充$\sigma=0.01$。对于分类子网的最终转换层，我们将偏差初始化设置为$b=-\log((1-\pi)/\pi)$，$\pi$指定训练开始时每个锚应该被标记为前景的置信度。我们用 $\pi= .01$在所有实验中，尽管结果对于确切值是稳健的。如3.3中所解释的，该初始化防止大量背景锚在第一次训练迭代中产生大的，不稳定的损失值。

**Optimization**：RetinaNet采用随机梯度下降（SGD）进行训练。我们使用8个GPU的同步SGD，每个小批量共有16个图像（每个GPU 2个图像）。除非另有说明，否则所有模型都训练为90k次迭代，初始学习率为0.01，然后在60k时再除以10，再在80k次迭代时除以10。除非另有说明，否则我们使用水平图像翻转作为数据增强的唯一形式。使用0.0001的重量衰减和0.9的动量。训练损失是用于框回归的焦点损失和标准平滑L1损失的总和[10]。表1e中的模型的训练时间在10到35小时之间。


### 5. Experiments

这一部分可以阅读原paper去了解，在此仅展示结果：

<div align=center>
<img src="zh-cn/img/retinanet/p10.png" />
</div>

*表2.对象检测单模型结果（边界框AP），与COCO test-dev的最新技术相比。我们展示了RetinaNet-101-800型号的结果，经过刻度抖动和1.5倍比表1e中的相同型号长。我们的模型取得了最佳成绩，优于单阶段和两阶段模型。有关速度与精度的详细分类，请参见表1e和图2。*


### 6. Conclusion

在这项工作中，我们将类不平衡确定为阻止one-stage目标检测超越最佳性能的two-stage方法的主要障碍。为了解决这个问题，提出了Focal Loss，它将调制项应用于交叉熵损失，以便将学习重点放在较难负例上。我们的方法简单而有效。通过设计一个完全卷积的one-stage目标检测模型RetinaNet来证明其有效性，实验结果表明它实现了最先进的精度和速度。源代码可在https://github.com/facebookresearch/Detectron[12]获得。
