## TDNN

<!-- https://www.chenzhengyang.cn/2019/03/20/paper-reading-tdnn/ -->

<!-- https://blog.csdn.net/qq_14962179/article/details/87926351 -->

<!-- https://blog.csdn.net/richard2357/article/details/16896837 -->

<!-- https://blog.csdn.net/w_manhong/article/details/100731180 -->

<!-- http://pelhans.com/2018/02/11/kaldi-note8/ -->
<!-- https://blog.csdn.net/richard2357/article/details/16896837 -->

近来在了解卷积神经网络（CNN），后来查到CNN是受语音信号处理中时延神经网络（TDNN）影响而发明的。本篇的大部分内容都来自关于TDNN原始文献[【1】](https://www.researchgate.net/publication/3175480_Phoneme_recognition_using_time-delay_neural_networks)的理解和整理。该文写与1989年，在识别"B", "D", "G"三个浊音中得到98.5%的准确率，高于HMM的93.7%。是CNN的先驱。

### 1.普通神经网络识别音素

在讲TDNN之前先说说一般的神经网络的是怎样识别音素的吧。假设要识别三个辅音"B", "D", "G"，那么我们可以设计这样的神经网络：

<div align=center>
    <img src="zh-cn/img/ch22/p1.png"   /> 
</div>

其中输入0-12代表每一帧的特征向量（如13维MFCC特征）。那么有人可能会问了，即使在同一个因素"B"中，比如"B"包含20帧，那么第1帧与第15帧的MFCC特征也可能不一样。这个模型合理吗？事实上，"B"包含的20帧MFCC特征虽然有可能不一样，但变化不会太大，对于因素还是有一定区分度的，也就是说这个模型凑合凑合还能用，但效果不会非常好。GMM模型可以用这种模型来解释。

### 2.时延神经网络（TDNN）

上述结构只考虑了一帧的特征，我们知道上下文信息对于序列模型是很有帮助的，我们需要考虑更多帧。当考虑延时为2时，则连续的3帧都会被考虑。其中隐藏层起到特征抽取的作用，输入层每一个矩形内共有13个小黑点，代表该帧的13维MFCC特征。假设每个隐藏层有10个节点，那么连接的权重的数目是`3 * 13* 10=390`个，用图表示为：

<div align=center>
    <img src="zh-cn/img/ch22/p2.png"   /> 
</div>

为了显示更多帧，紧凑表示为：

<div align=center>
    <img src="zh-cn/img/ch22/p3.png"   /> 
</div>

其中一条彩色线就代表`13*10=130`个权重值,三条彩色线为390个权重。也有资料称之为滤波器。如果时间滚滚向前，我们不断地对语音帧使用滤波器，我们可以得到下图：

<div align=center>
    <img src="zh-cn/img/ch22/p4.png"   /> 
</div>

这就是延时神经网络的精髓了！其中绿色的线权值相同，红色的线权值相同，蓝色的线权值相同。相当于把滤波器延时。输入与隐层共390个权值变量待确定。

每个隐层矩形内包含10个节点，那么每条棕色的线包含10个权值，假设输出层与隐层的延时为4，则接收5个隐层矩形内的数据，那么隐层与输出层合计权值为`10*5*3=150`。权值非常少！所以便于训练。

下面就不难理解文献【1】上的图了。思想与上文一样，不过文章多用了一层隐层（多隐层有更强的特征提取和抽象能力）

<div align=center>
    <img src="zh-cn/img/ch22/p5.png"   /> 
</div>

介绍一下他的做法。Input Layer为语谱图，黑块为大值，灰块为小值。输入层纵向为经过mel滤波器的16个特征（没用MFCC），横向为帧。Input Layer 的延时为2，映射到Hidden Layer 1的关系为`16*3 -> 8`，权值个数为384。Hidden Layer 1 的延时为4，映射到Hidden Layer 2的关系为`8*5 -> 3`，权值个数为120。Hidden Layer 2 的延时为8，映射到输出层的关系为`3*9 -> 3`，权值个数为81。合计权值为`384+120+81=585`。输出的三个单元分别代表"B", "D", "G"的得分。

### 3.训练方法及小结

+ TDNN训练方法

a.和传统的反向传播算法一样；

b.TDNN有快速算法,有兴趣的读者可以搜索。

+ 总结TDNN网络的优点：

a.网络是多层的，每层对特征有较强的抽象能力；

b.能表达语音特征在时间上的关系；

c.权值具有时间不变性；

d.学习过程中不要求对所学的标记进行精确的时间定位；

e.通过共享权值，方便学习。

### 4. A time delay neural network architecture for efficient modeling of long temporal contexts 2015 Danial Povey, interspeech

这篇文章所介绍的TDNN结构是在之前提出的TDNN结构上的一种改进，虽然TDNN是一种前向网络的架构，但是如果在所有的time steps上计算隐层的激活计算量仍然是很大的。在本文中，作者采用了一种subsampling的方式，只对某些time step进行计算，这中subsampling是建立在所有的输入上下文都会被网络处理的基础上的。

对于一般的DNN在处理上下文时，想法一般是这样的。比如我们想提取具有上下文分别7帧共15帧的特征表达，我们一般会将这15帧的特征直接拼起来，形成一个`15*F`（F是我们每一帧的特征维度）的特征，然后去学习`15*F`的特征映射。

而对于TDNN来说不是这样做的，假如我们最后仍然想获取时序上15帧上下文的特征表达。在TDNN的初始层中，会处理比15帧更加窄的时序上下文，然后送入更深的网络。很容易理解，更深的网络的时间分辨率是比底层网络要长的。也就是说，我们将“获取时序上15帧上下文的特征”这个目标交给了更深的网络去完成，而不是用一层网络来完成。 如下图，最下面一层的时间分辨率为5，而最上层的时间分别率是23。

<div align=center>
    <img src="zh-cn/img/ch22/p6.png"   /> 
</div>

此外，TDNN在同一层，不同time steps上的这个transform参数都是共享的（类似于CNN中的卷积核在整张map上的参数共享）。所以对于TDNN来说，其中的一个超参数就是每一层的input context。

RNN可以对时间有效地对时间依赖帧进行建模，但是由于训练时的时间序列性，训练时间比较长。RNN并行化程度不如前向神经网络，而TDNN也可以对长时依赖序列建模，并使用子采样来减少计算量，可以让TDNN训练时间和标准前向神经网络差不多。

**TDNN subsampling**

从上图可以看出，TDNN临近time step的特征（非输入层）是有很大的context重叠的。假定这些相邻的time step特征之间是相关的，我们在进行特征拼接输入下一层的时候是不必拼接紧挨着的time step的。也就是说我们可以隔几个time step进行拼接。这些做的话，如上图，红线所示的路径就是我们最后需要计算的部分。在原始的TDNN是一个全连接的前馈神经网络。但事实上，相邻时间点所包含的上下文信息有很大部分重叠的，因此可以采用subsampling的方法，只保留部分的连线，可以获得原始模型近似的效果，同时能够大大减小模型的计算量。子采样拼接帧不超过2

此外在这个文章的TDNN的结构中，用的是非对称的上下文，也就是说当前帧之前取13帧，当前帧之后取9帧，这可以降低实时解码的延迟（减少对于未来帧的使用）。使用非对称输入上下文，左侧有更多的上下文，是为了减少在线解码过程中神经网络的延时。也能减少字错误率。
非对称上下文窗口一般是从过去的16帧到之后的9帧这个区间内。使用L2范数。

**输入特征**

MFCC特征和对数梅尔特征。
在每帧40维MFCC特征的基础上，增加100维`i-vector`特征。其中，MFCC不用做CMVN，目的时使`i-vector`可以提供说话人数据的均值偏移信息。

**DNN训练**

分层监督训练，用随机梯度下降处理更新，使用指数学习率。DNN并行设置最多达18 GPU完成，使用模型平均技术。

**序列区分性训练**

基于状态级别的最小音素错误(MPE),称为sMBR准则用来完成DNN上的序列训练。

其测试结果如下：

<div align=center>
    <img src="zh-cn/img/ch22/p7.png"   /> 
</div>

<div align=center>
    <img src="zh-cn/img/ch22/p8.png"   /> 
</div>

<div align=center>
    <img src="zh-cn/img/ch22/p9.png"   /> 
</div>


**结论**

TDNN在处理较宽的上下文输入时很有效。输入上下文的宽度是`[t-13,t+9]`时是最优的。使用子采样技术在训练是进行加速。与DNN相比，性能相对提升6%，比使用说话人自适应特征的RNN结构相对好2.6%。下一步从p范数非线性切换到Relu，据说在TDNN上很好用。