{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n#这段是kaggle用于帮用户展示 比赛数据路径、自己加载的数据路径等","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/input/spooky-author-identification/test.zip\n/kaggle/input/spooky-author-identification/sample_submission.zip\n/kaggle/input/spooky-author-identification/train.zip\n/kaggle/input/uncased-l12-h768-a12/uncased_L-12_H-768_A-12/bert_model.ckpt.meta\n/kaggle/input/uncased-l12-h768-a12/uncased_L-12_H-768_A-12/vocab.txt\n/kaggle/input/uncased-l12-h768-a12/uncased_L-12_H-768_A-12/bert_model.ckpt.index\n/kaggle/input/uncased-l12-h768-a12/uncased_L-12_H-768_A-12/bert_config.json\n/kaggle/input/uncased-l12-h768-a12/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\n/kaggle/input/uncased-l12-h768-a12/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_model.ckpt.meta\n/kaggle/input/uncased-l12-h768-a12/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/vocab.txt\n/kaggle/input/uncased-l12-h768-a12/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_model.ckpt.index\n/kaggle/input/uncased-l12-h768-a12/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_config.json\n/kaggle/input/uncased-l12-h768-a12/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_model.ckpt.data-00000-of-00001\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"#安装keras_bert\n!pip install keras_bert","execution_count":4,"outputs":[{"output_type":"stream","text":"Collecting keras_bert\n  Downloading keras-bert-0.81.0.tar.gz (29 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.6/site-packages (from keras_bert) (1.18.2)\nRequirement already satisfied: Keras in /opt/conda/lib/python3.6/site-packages (from keras_bert) (2.3.1)\nCollecting keras-transformer>=0.30.0\n  Downloading keras-transformer-0.32.0.tar.gz (11 kB)\nRequirement already satisfied: scipy>=0.14 in /opt/conda/lib/python3.6/site-packages (from Keras->keras_bert) (1.4.1)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from Keras->keras_bert) (5.3.1)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.6/site-packages (from Keras->keras_bert) (1.14.0)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /opt/conda/lib/python3.6/site-packages (from Keras->keras_bert) (1.1.0)\nRequirement already satisfied: keras-applications>=1.0.6 in /opt/conda/lib/python3.6/site-packages (from Keras->keras_bert) (1.0.8)\nRequirement already satisfied: h5py in /opt/conda/lib/python3.6/site-packages (from Keras->keras_bert) (2.10.0)\nCollecting keras-pos-embd>=0.10.0\n  Downloading keras-pos-embd-0.11.0.tar.gz (5.9 kB)\nCollecting keras-multi-head>=0.22.0\n  Downloading keras-multi-head-0.22.0.tar.gz (12 kB)\nCollecting keras-layer-normalization>=0.12.0\n  Downloading keras-layer-normalization-0.14.0.tar.gz (4.3 kB)\nCollecting keras-position-wise-feed-forward>=0.5.0\n  Downloading keras-position-wise-feed-forward-0.6.0.tar.gz (4.4 kB)\nCollecting keras-embed-sim>=0.7.0\n  Downloading keras-embed-sim-0.7.0.tar.gz (4.1 kB)\nCollecting keras-self-attention==0.41.0\n  Downloading keras-self-attention-0.41.0.tar.gz (9.3 kB)\nBuilding wheels for collected packages: keras-bert, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n  Building wheel for keras-bert (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-bert: filename=keras_bert-0.81.0-py3-none-any.whl size=37912 sha256=d33200c8086716ce87a95898bf4d7603ec0477348269828b1af61e4dea32bac1\n  Stored in directory: /root/.cache/pip/wheels/ee/d9/2a/75b40df359ab9096f06e55804ca64fbb2592a6ff77345c5fa7\n  Building wheel for keras-transformer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-transformer: filename=keras_transformer-0.32.0-py3-none-any.whl size=13265 sha256=066ee75cfe2245c6842e57f2d00b272300f7889e2032366e705d89b52fc32f58\n  Stored in directory: /root/.cache/pip/wheels/45/b4/8b/5bc34b8f664af4f40fd27cc6c302ee3d6c7f29f181686f7ba5\n  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-py3-none-any.whl size=7553 sha256=f039b01397f1096b97450a9d01fee2abd3496b7402acd6ca2963d267b6391748\n  Stored in directory: /root/.cache/pip/wheels/13/b1/3b/13b632f78162148b123cddad1e0e3786df45ec37cac86dd998\n  Building wheel for keras-multi-head (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-py3-none-any.whl size=15373 sha256=c1b7b2ff7558a041a18f63ce2c9c991c2a56e83eb35dc5ab0d578d0e559d1bd0\n  Stored in directory: /root/.cache/pip/wheels/43/20/55/ac730d8966dcc6fab002f2bf9be5eefecaae9eda8661586f2d\n  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-py3-none-any.whl size=5267 sha256=8cb7cbc81a2cd7a1650320fb6038db01e410a33cba16d16bbd190193634f51cc\n  Stored in directory: /root/.cache/pip/wheels/60/1a/38/858ffe627cf272dc54d9863d6c5ec993f00fd28d33f7f169f8\n  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-py3-none-any.whl size=5623 sha256=183e4a45c8f607859aa85b42f1c5936b1373ecccd3495caace3d16dccd7f16cb\n  Stored in directory: /root/.cache/pip/wheels/75/25/c7/5a4c25eabcddaa3f108a9fe5ad8f0ad94e6566f25c391ea4f6\n  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-py3-none-any.whl size=4674 sha256=b75dfdbce047a5fda0edabb330bb0b3c4f9f3627a374a1bb3c24334f8010631f\n  Stored in directory: /root/.cache/pip/wheels/f3/16/a7/275994b075e49c199afced51712534a142429c90cd92a19241\n  Building wheel for keras-self-attention (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-py3-none-any.whl size=17288 sha256=6f3333b2fa1d607f3a0b4c7acf95f91b2b9a61373682693cd7a1bc8e581eea28\n  Stored in directory: /root/.cache/pip/wheels/dc/71/4f/03604d0ee00490bf16606901a5977cd2bc3e4a087aff710e4f\nSuccessfully built keras-bert keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\nInstalling collected packages: keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert\nSuccessfully installed keras-bert-0.81.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.32.0\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras_bert import load_trained_model_from_checkpoint, Tokenizer\nfrom keras.layers import Lambda, Input, Dense\nfrom keras.models import Model\nfrom sklearn.model_selection import train_test_split\nfrom zipfile import ZipFile\nimport pandas as pd\nimport codecs\nimport nltk\n\nconfig_path = '/kaggle/input/uncased-l12-h768-a12/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_config.json'\ncheckpoint_path = '/kaggle/input/uncased-l12-h768-a12/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/bert_model.ckpt'\ndict_path = '/kaggle/input/uncased-l12-h768-a12/uncased_l-12_h-768_a-12/uncased_L-12_H-768_A-12/vocab.txt'\ntrain_path = '/kaggle/input/spooky-author-identification/train.zip'\ntrain_path_csv = '/kaggle/input/spooky-author-identification/train.csv'\ntest_path = '/kaggle/input/spooky-author-identification/test.zip'\n","execution_count":5,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"#重写一下keras_bert的Tokenizer（不重写问题也不大）。重写的目的是为了解决，当遇到未登陆词(即不在bert词库中)时，打上[UNK]标签。\ntoken_dict = {}\n\nwith codecs.open(dict_path, 'r', 'utf8') as reader:\n    for line in reader:\n        token = line.strip()\n        token_dict[token] = len(token_dict)\n        \nclass OurTokenizer(Tokenizer):\n    def _tokenize(self, text):\n        R = []\n        for c in text:\n            if c in self._token_dict:\n                R.append(c)\n            else:\n                R.append('[UNK]')\n                \n        return R\n    \ntokenizer = OurTokenizer(token_dict)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#大家可以看一下bert的tokenizer的作用。\nx1, x2 = tokenizer.encode(['I', 'am', 'sad', '!'])\nprint(x1)\nprint(x2)\n#输出为\n# x1:[101, 100, 2572, 6517, 999, 102]\n# x2:[0, 0, 0, 0, 0, 0]\n\n#本来长度为4的句子，为什么encode之后的长度为6呢？\n#这是因为bert会给输入的序列首位分别加上<CLS>和<SEP>。<CLS>可以用来表示整个句子的向量\n#x2全是0，是因为bert的输入有两个，本任务只是做文本分类，用不到x2，因此全为0.当做QA任务时，x1可以是问句的encoding，x2是答案的encoding","execution_count":9,"outputs":[{"output_type":"stream","text":"[101, 100, 2572, 6517, 999, 102]\n[0, 0, 0, 0, 0, 0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"myzip=ZipFile(train_path)              #因为数据集被打包成了zip，因此用此骚操作读取里面被压缩的csv数据\nwith myzip.open('train.csv') as f:\n    df=pd.read_csv(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"myzip=ZipFile(test_path)             #读取测试集\nwith myzip.open('test.csv') as f:\n    test_df=pd.read_csv(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#label ID化\nauthor_dict = {author: index for index, author in enumerate(df['author'].unique())}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#这个dict是为了帮助在预测时，将预测当id转回对应当author\nid2author = {v: k for k, v in author_dict.items()}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#重头戏来了，加载bert模型。一句代码就搞定\nbert_model = load_trained_model_from_checkpoint(config_path, checkpoint_path, seq_len=None)\n\n#因为bert有许多曾，使用下面当代码，将每一层的trainable设置为true时，每一层的参数在训练过程中都会被修改。否则，bert预训练模型的参数，在训练过程中不会被修改。\nfor l in bert_model.layers:\n    l.trainable = True","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_x_len = 100\nnum_class = 3\n\n#搭建模型。因为bert一定要有两个输入。所以有x1和x2\n\nx1_in = Input(shape=(max_x_len,), dtype='int32')\nx2_in = Input(shape=(max_x_len,), dtype='int32')\n\nx = bert_model([x1_in, x2_in])\nx4cls = Lambda(lambda x: x[:, 0])(x)  #取出每个样本的第一个向量。因为bert输入的第一个向量是<cls>，这个向量可以用来文本分类。\n\nout = Dense(num_class, activation='softmax')(x4cls)\n\nmodel = Model([x1_in, x2_in], out)\n\nmodel.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gen_data(df, train=True):\n    text = list(df['text'])\n    idx = list(df['id'])\n    author = []\n    if train:\n        author = list(df['author'].map(author_dict))\n  \n    X1, X2 = [], []\n    for item in text:\n        x1, x2 = tokenizer.encode(nltk.word_tokenize(item))\n        if len(x1)>max_x_len:\n            x1 = x1[:max_x_len]\n            x2 = x2[:max_x_len]  #x2是全0向量\n        else:\n            x1 = x1 + [0] * (max_x_len-len(x1))\n            x2 = x2 + [0] * (max_x_len-len(x2))\n            \n        X1.append(x1)\n        X2.append(x2)\n        \n    if train:\n        return np.array(X1), np.array(X2), np.array(author)\n    else:\n        return idx, X1, X2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X1, X2, author = gen_data(df)\nidx, test_x1, test_x2 = gen_data(test_df, False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"author = np.expand_dims(author, axis=-1)\nnp.expand_dims\nprint(X1.shape)\nprint(X2.shape)\nprint(author.shape)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_x1, val_x1, train_x2, val_x2, train_y, val_y = train_test_split(X1, X2, author, test_size=0.4, random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit([train_x1, train_x2], train_y, validation_data=([val_x1, val_x2],val_y), epochs=2, batch_size=64)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict([test_x1, test_x2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"author_dict","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out = pd.DataFrame(columns=(['id','EAP','HPL','MWS']))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out['id'] = idx\nout[['EAP','HPL','MWS']] = result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out.to_csv('result.csv', index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"out.head()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}