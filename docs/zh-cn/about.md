
## 关于我们

+ 个人主页:https://dataxujing.github.io/

------

**个人介绍：**

!> **徐静** （[dataxujing](https://github.com/DataXujing)） AI图像算法研发工程师，硕士研究生，数据科学爱好者，喜欢R, Python, C++ 关注机器学习，深度学习等AI领域； 喜欢网络爬虫，关注前端可视化; 对ASR，NLP，CV均有涉猎；
目前从事医疗影像AI算法的研究和落地工作。个人还是个小白，希望多交流！

------

**更新日志**

|    日期              |   更新内容                |
|:--------------------:|:------------------------:|
|2020-05-23            |  word2vec                |
|2020-05-27            |  GloVe                   |
|2020-05-28            |  fastText                |
|2020-05-29            |  textCNN                 |
|2020-06-01            |  LDA                     |
|2020-06-01            |  LSA(LSI)                |
|2020-06-05            |  pLSA(pLSI)              |
|2020-06-05            |  文本建模                 |
|2020-06-12            |  lda2vec                 |
|2020-06-22            |  CRF                     |
|2020-07-23            |  PageRank                |
|2020-07-25            |  CRF                     |
|2020-08-2             |  elmo                    |
|2020-08-5             |  seq2seq                 |
|2020-08-6             |  attention机制            |
|2020-08-12            |  Transformer             |
|2020-08-17            |  Vanilla Transformer     |
|2020-08-17            |  Transformer-XL          |
|2020-08-19            |  DETR                    |
|2020-08-28            |  BERT                    |
|2020-08-30            |  ALBERT                  |
|2020-09-01            |  XLNet                   |
|2020-09-02            |  GPT-1                   |
|2020-09-03            |  GPT-2                   |
|2020-09-04            |  GPT-3                   |
|2020-09-10            |  RoBERTa                 |
|2020-09-19            |  MASS                    |
|2020-09-29            |  BI-LSTM-CRF             |
|2020-10-06            |  ERNIE                   |
|2020-10-07            |  NEZHA                   |
|2020-10-09            |  BERT-WWM                |
|2020-10-10            |  SpanBERT                |
|2021-03-15            |  DeBerta                 |
|2021-09-17 | ChineseBERT |






**TODO**

近两年关于Transformer和BERT相关的魔改非常非常多，但是当你熟悉了上述更新的内容后，其他方法就变得简单。

未来我们根据时间安排，也许会更新如下其他NLP相关算法的详细讲解和paper解读及coding案例(注：我们提供了paper的下载地址和完整的paper,标\*建议必读)

* [ ] Bert与模型蒸馏：DistillBert\*
* [ ] Tiny-Bert:模型蒸馏的全方位应用
* [ ] MobileBert: Pixel4上推算速度仅需要40ms
* [x] SpanBert\*
* [ ] VisualBERT
* [ ] ViLBERT
* [ ] VideoBERT
* [ ] Bert与AutoML
* [x] RoBerta\*
* [ ] Transformer优化之自适应宽度注意力
* [ ] Reformer: 局部敏感哈希和可逆残差带来的高效
* [ ] Longformer: 局部attentoin和全局attention的混搭
* [ ] Linformer
* [ ] T5\*
* [x] MASS\*
* [x] ERNIE\*,ERNIE2\*
* [x] BERT-wwm\*,BERT-wwm-ext\*,RoBERTa-wwm-ext\*,RoBERTa-wwm-large\*
* [x] NEZHA\*

























