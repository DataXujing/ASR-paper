## DenseNet

------


### 0.摘要

最近的成果显示，如果神经网络各层到输入和输出层采用更短的连接，那么网络可以设计的更深、更准确且训练起来更有效率。本文根据这个现象，提出了Dense Convolutional Network (DenseNet)，它以前馈的方式将每个层都连接到其他每一层。然而传统L层卷积网络有L连接，而DenseNet的任一层不仅与相邻层有连接，而且与它的随后的所有层都有直接连接，所以该网络有L(L+1)/2个直接连接。DenseNets有如下几个令人信服的优点：缓解了消失梯度问题，增强了特征传播，促进了特征再用，大大减少了参数的数量。本文计算架构在四个具有高度竞争的目标识别数据集上进行实验。DenseNets相对于目前最先进的算法有明显的改善，且采用更少的计算来实现高性能。代码和预训练模型如下<https://github.com/liuzhuang13/DenseNet>.

### 1.介绍

卷积神经网络（CNN）已成为占主导地位的机器学习的视觉物体识别方法。尽管CNN最早被提出已经有20年了，但计算机硬件和网络结构的改进才使得最近才能真正的训练深度神经网络。原始的LeNet5包含了5层，VGG网络19层，Highway和ResNets超过了100层的大关。 

随着神经网络变得越来越深，一个新的研究问题出现了：当输入或梯度信息经过许多层时，当它到达网络的结束（或开始）时，它就会消失并“洗出”。 许多最近的文章着力解决这个问题，ResNets [11] 和 Highway Networks [33] 旁路信号通过身份连接从一层到下一层。Stochastic depth[ 13 ]缩短resnets是通过训练期间随机丢弃层，来获得更好的信息和梯度流。FractalNets [17]将不同数量的卷积块的多个并行层序列重复组合，以获得较大的标称深度，同时在网络中保持许多短路径。尽管这些不同的方法在网络拓扑结构和训练过程中有所不同，但它们都具有一个关键特征：它们创建了从前面层到后续层的短路径。

在本文中，我们提出了一种将这种方法提炼成简单连接模式的架构：为了确保网络中各层之间的最大信息流，我们直接将所有层（具有匹配的特征映射大小）连接起来。为了保留前向传播的特性，每个层从前面的所有层获得附加输入，并将其自身的特征映射传递到所有后续层。如图一所示。


<div align=center>
<img src="zh-cn/img/DenseNet/p1.jpg" />
</div>

重要的一点，与ResNets不同的是，我们不是在特征传递给某一层之前将其进行相加（combine），而是将其进行拼接（concatenate）。因此，第 l 层有 l  个输入，这些输入是该层之前的所有卷积块（block）的特征图，而它自己的特征图则传递给之后的所有 L-l 层。这就表示，一个 L 层的网络就有 L（L+1）/2 个连接，而不是像传统的结构仅仅有 L 个连接。由于它的稠密连接模块，所以我们更喜欢把这个方法称为稠密卷积网络（DenseNets）.

至关重要的是，与ResNets相比，我们没有采用合并之前所有层的特征到一层的方法，相反，我们把它们连接起来结合所有层的特点。因此，该ℓ层有ℓ个输入，由之前的所有卷积块的特征图组成，它自己的特征映射被传递到所有的L-ℓ后续层。这在L层网络中引入连接，而不像传统体系结构中的L那样。 由于其密集的连接模式，我们将我们的方法称为密集卷积网络（DenseNet）。一个可能这种密集的连接模式的直观效果是它需要的参数比传统的卷积网络少，无需重新学习冗余特征图。传统的前馈结构可以看作是一种状态的算法，它是从层到层传递的。每个层从其上一层读取状态并写入后续层，它改变了状态，但也传递了需要保存的信息。 ResNets [11]清楚地保存信息是通过附加的标识转换。ResNets [13]最近的改进表明，有很多层的贡献很少，实际上可以在训练期间随机丢弃，使得ResNet的状态类似于（展开的）递归神经网络[21]，但是ResNets的参数数目要大得多，因为每一个层都有自己的权重。我们提出的DenseNet体系结构清楚的区分添加到网络的信息和保存的信息。DenseNet层很窄（例如，每层12个filter），添加少量的特征图的“集体知识”的网络，保持其余的特征图不变，最终的分类器的结果是基于网络的所有特征图。除了更好的参数效率之外，DenseNets的一大优势是改善了整个网络中的信息流和梯度，这使得它们易于训练。每一层都可以直接从损失函数和原始输入信号中获得梯度，从而产生隐含的深层监督[20]。这有助于深入网络体系结构的训练。 此外，我们还观察到密集连接具有正则化效应，缓解了训练集小导致的过拟合现象。 

我们在四个目标检测任务（CIFAR-10，CIFAR-100，SVHN和ImageNet）中验证了DenseNets。在和现有模型有相似准确率的前提下，我们的模型有更少的参数。此外，我们的网络还超过了目前在大部分的检测任务都有最好结果的算法。

### 2.相关工作

自从神经网络被提出之后，网络结构的探索就成为了神经网络研究的一部分。最近神经网络的广泛关注也给这个研究领域注入了新的生机。网络层数的增加也让更多的人进行结构的改善、不同连接模式的探索、早期研究观点的复现等方面的研究。

在1980s神经网络论文中提出的级联结构很像我们提出的稠密网络。他们之前的工作主要关注在全连接的多层感知机上。最近，使用批梯度下降训练的全连接的级联网络也被提出来了。尽管在小数据集上有效，但该方法的网络却有几百个参数。[9，23，30，40]提出在CNNs中利用跨层连接获得的多种特征，这已经被证明在很多视觉任务上有效。和我们的工作类似，[1]使用和我们相似的跨层连接方式提出了一种纯理论的网络框架。

HighWay是这些网络中第一个提出使用100多层的结构训练一个端到端的网络。使用旁路（bypassing paths）和门控单元（gating units），HighWay网络可以很轻松的优化上百层的网络。旁路被认为是使深层网络容易训练关键因素。该观点在ResNets中被进一步证实，ResNets使用本身的特征图作为旁路。ResNets在很多图像识别、定位和检测任务（如ImageNet和COCO目标检测）中都获得了不错的效果，并且还打破了之前的记录。最近，一种可以成功训练1202层ResNet的随机深度（stochastic depth）被提出。随机深度通过在训练过程中随机丢掉一些层来优化深度残差网络的训练过程。这表明深度（残差）网络中并不是所有的层都是必要的，有很多层是冗余的。我们论文的一部分就受到了该结论的启发。预激活（pre-activation）的ResNets也有助于训练超过1000层的网络。

一种让网络更深（如跨层连接）的正交法（orthogonal approach）是增加网络的宽度。GooLeNet使用了“inception”模块，将不同尺寸的滤波器产生的特征进行组合连接。在[37]中，提出一种具有广泛宽度的残差模块，它是ResNets的一种变形。事实上，只简单的增加ResNets每一层的滤波器个数就可以提升网络的性能。FractalNets使用一个宽的网络结构在一些数据集上也获得了不错的效果。

DenseNets不是通过很深或者很宽的网络来获得表征能力，而是通过特征的重复使用来利用网络的隐含信息，获得更容易训练、参数效率更高的稠密模型。将不同层学到的特征图进行组合连接，增加了之后层输入的多样性，提升了性能。这同时也指出了DenseNets和ResNets之间的主要差异。尽管inception网络也组合连接了不同层的特征，但DenseNets更简单，也更高效。

也有很多著名的网络结构获得了不错的结果。NIN结构将多层感知机与卷积层的滤波器相连来提取更复杂的特征。在DSN中，通过辅助分类器来监督内部层，加强了前几层的梯度。Ladder网络将横向连接（lateral connection）引入到自编码器中，在半监督学习任务中获得不错的效果。在[38]中，DFNs通过连接不同基础网络的中间层来改善信息的传递。带有可以最小化重建损失路径（pathways that minimize reconstruction losses）的网络也可以改善图像分类模型的性能。

### 3.DenseNet


考虑一张单一的图像x0通过一个卷积神经网络。这个网络有L层，每层实现一个非线性变换Hℓ(•),ℓ是层的编号，Hℓ(•)可以是批量归一化的复合函数（例如BN），rectified线性单元 (ReLU)，池化或者卷积，Xℓ作为ℓ层的输出.

ResNets. 传统的卷积是把前馈网络ℓ层的输出作为（ℓ+ 1）层的输入[16]，产生了如下的转换关系：Xℓ= H（Xℓ−1）。ResNets [11]添加一个跳过连接绕过非线性变换的特征函数： 
<div align=center>
xℓ = Hℓ(xℓ−1) + xℓ−1. 
</div>

ResNets的优点是梯度直接通过特征函数从后面层流向前面的层，然而，特征函数和Hℓ输出求和可能阻碍网络中的信息流。 

**稠密连接**，为了进一步改善层之间的信息流，我们提出了不同的连接模式：我们引入了从任何层到所有后续层的直接连接。 图1示意性地示出了由此产生的DenseNet的布局。 因此，第ℓ层接收所有前面的层x0，…，Xℓ−1的特征图作为输入： 
<div align=center>
xℓ = Hℓ([x0,x1,…,xℓ−1]), (2) 
</div>
其中[x0，x1，…，xℓ-1]是指在层0，…，ℓ-1中产生的特征图的拼接。 由于密集的连接性，我们把这种网络结构称为密集卷积网络（DenseNet）。Hℓ（•）的多输入在式（2）为一个张量。（这部分的意思是说，Hℓ的输入是前面0到ℓ-1个X特征图）。

**组合函数**，由[12]推导，我们定义H l（•）为三个连续操作的复合函数：批量归一化（BN）[14]，其次是（ReLU）[6]和3×3卷积（Conv）

**池化层**，方程（2）中使用的连接操作在特征图的大小改变时是不可行的。（个人理解是，第一步的dense connectivity，如果特征图的尺寸不一致，不能进行连接操作的） 然而，卷积网络的一个重要组成部分是下采样层，它改变了特征图的大小。 为了简化体系结构中的下采样，我们将网络划分为多个密集连接的密集块; 见图2。我们将块之间的层称为过渡层（transition layers），对它们做卷积和池化。 在我们的实验中使用的过渡层由batch normalization层和1×1卷积层，然后是2×2平均池化层组成。 

<div align=center>
<img src="zh-cn/img/DenseNet/p2.png" />
</div>

**增长速率（growth rate）**，如果每个函数Hℓ产生k个 featuremaps，由此可见，ℓ层有K0 + K×（ℓ−1）输入特征图，其中K0是输入层的通道数（如果是初始的RGB，就是K0就是3）。DenseNet和现有的网络体系结构的一个重要的区别是，DenseNet可以有很窄的层，例如，K = 12。我们将超参数K作为网络的增长率。我们在第4节中提到，一个相对较小的增长率足以在我们测试的数据集上获得了最先进的结果。对此的一个解释是，每一层都可以访问其块中的所有前面的特征图，因此可以访问网络的“集体知识”。 可以将特征图视为网络的全局状态。 每个层都将自己的k个特征图添加到这个状态。 增长率规定了每个层对全局状态贡献的新信息量。 全局状态一旦写入，就可以从网络中的任何地方访问，与传统的网络体系结构不同的是，并不需要一层一层地复制它。

**Bottleneck层**，虽然每个层只输出k个特征图，但它通常有更多的输入。在[ 36, 11 ] 已经指出，1×1卷积可以作为Bottleneck layers在每3×3卷积来减少输入特征图的数量（这部分是降维），从而提高计算效率。我们发现这个设计对于DenseNet特别有效，我们把这个Bottleneck layers称为我们的网络，即对于H层的BN-ReLU-Conv（1×1）-BN-ReLU-Conv（3×3）版本，DenseNet-B。 在我们的实验中，我们让每个1×1卷积产生4k个特征图。 

**Compression**，为了进一步提高模型的紧凑性，我们可以减少过渡层的特征图数量。 如果一个密集块包含m个特征图，我们让下面的过渡层产生⌊θm⌋输出特征图，其中0 <θ≤1被称为压缩因子。 当θ= 1时，过渡层上的特征图的数量保持不变。 我们称DenseNet的θ<1为DenseNet-C，在实验中设定θ= 0.5。 当使用θ<1的瓶颈和过渡层时，我们将模型称为DenseNet-BC。 


**实现细节**，在除ImageNet以外的所有数据集上，我们实验中使用的DenseNet有三个密集块，每个块都有相同数量的层。在进入第一密集块之前，对输入图像执行16（或DenseNet-BC增长率的两倍）输出通道的卷积。 对于卷积核大小为3×3的卷积层，输入的每一边都被填充一个像素以保持特征图大小的不变。我们使用1×1卷积，然后使用2×2平均池化作为两个连续密集块之间的过渡层。 在最后一个密集块的末尾，采用一个全局平均池化，然后附加一个softmax分类器。三个密集块中的特征图大小分别是32×32,16×16和8×8。 我们试验了配置分别为{L = 40，k = 12}，{L = 100，k = 12}和{L = 100，k = 24}的基本DenseNet结构。对于DenseNetBC，评估配置为{L = 100，k = 12}，{L = 250，k = 24}和{L = 190，k = 40}的网络。 
在ImageNet的实验中，我们使用了DenseNet-BC结构，在224×224的输入图像上有4个密集块。 初始卷积层包括为步长为2的7×7卷积的2k个;所有其他层中的特征图的数量设置为k。我们在ImageNet上使用的确切网络配置如表1所示。 


<div align=center>
<img src="zh-cn/img/DenseNet/p3.png" />
</div>

### 4.实验

我们在一些检测任务的数据集上证明DenseNet的有效性，并且和现有的一些网络进行了对比，特别是ResNet和它的变形。

#### 4.1 数据集

**CIFAR**。两种CIFAR数据集都是32x32的彩色图。CIFAR-10(C10)是10类，CIFAR-100(C100)是100类。训练集和测试集分别有50000和10000张图片，我们从训练集中选5000张作为验证集。我们采用在这两个数据集上广泛使用的数据增强方式（镜像/平移）。用在数据集后的“+”来表示使用了这种数据增强方式（如C10+）。至于预处理，我们使用每个颜色通道的均值和标准差来归一化。最后，我们使用全部的50000张训练图片，在训练结束时记录测试误差。

**SVHN**。SVHN数据集是32x32的彩色数字图。训练集有73257张图片，测试集有26032张，有531131张作为额外的训练。在接下来实验中，我们没有使用任何的数据增强，从训练集中选取6000张图片作为验证集。我们用验证集误差最小的模型来进行测试。我们对像素值执行除255操作，归一化到[0,1]。

**ImageNet**。ILSVARC 2012分类数据集有1.2百万张训练集，50000张验证集，共1000类。我们采用和论文[8,11,12]同样的数据增强方式，在测试时使用single-crop或10-crop将图片尺寸变为224x224。根据[11,12,13]，我们记录了在验证集上的分类误差。

#### 4.2 训练

所有的网络都使用随机梯度下降（SGD）进行训练。 在CIFAR和SVHN上，我们用64 batch size 分别训练300和40轮。 最初的学习率设定为0.1，在训练时期总数中的50％和75％除以10。 在ImageNet上，采用batch size为256训练90轮。学习速率初始设置为0.1，在30和60轮时降低10倍。由于GPU内存的限制，我们最大的型号（DenseNet-161）以mini-batch size 128进行训练。为了弥补较小batch size，我们训练这个模型100轮，在90轮时采用除以10的学习率. 
我们使用10-4的权重衰减和0.9的Nesterov动量[34]，其中Nesterov动量不衰减。 我们采用由[10]引入的权重初始化。 对于没有数据增强的三个数据集，即C10，C100和SVHN，我们在每个卷积层（除了第一个卷积层）之后添加一个丢失层[32]，并将丢失率设置为0.2。 测试错误仅针对每个任务和模型设置评估一次。 

#### 4.3 在CIFAR和SVHN上的分类结果

我们用不同的深度L和增长率k来训练DenseNets。 表2列出了CIFAR和SVHN的主要结果。为了突出总体趋势，我们将超过现有技术水平的以粗体显示，并以蓝色表示最好的结果。

<div align=center>
<img src="zh-cn/img/DenseNet/p4.png" />
</div>

**准确率**。可能最明显的趋势可能起源于表2的最后一行，这表明在所有CIFAR数据集上，具有L = 190和k = 40的DenseNet-BC一致地优于现有技术水平。 它在C10 +上的错误率为3.46％，在C100 +上的错误率为17.18％，明显低于ResNet架构[41]的错误率。我们在C10和C100上的最好结果（没有数据增加）更令人鼓舞：两者都比FractalNet低30％，并且具有下降路径正则化[17]。 在SVHN上，当丢失层时，L = 100和k = 24的DenseNet也超过了ResNet所取得的最好结果。 然而，250层的DenseNet-BC并没有进一步改善其性能。 这可以解释为SVHN是一个相对容易的任务，而且深的模型可能会产生过拟合现象。 

**容量（capacity）**。在没有压缩或Bottleneck layers的情况下，随着L和k增加，DenseNets表现出更好的性能。 我们把这主要归因于模型能力的相应增长。 这最好由C10 +和C100 +列表示。 在C10 +上，随着参数从1.0M增加到7.0M，再增加到27.2M，误差从5.24％下降到4.10％，最终下降到3.74％。在C100 +上，我们观察到了类似的趋势。 这表明DenseNets可以利用越来越深的模型的代表性力量。 这也表明它们不会出现过拟合或残余网络的优化困难[11]。 

**参数效率**。表2的结果表明，DenseNets利用参数比其他结构更有效（特别是ResNets）。在过渡层的瓶颈结构和降维DenseNet-BC特别有效。例如，我们的250层模型只有15.3M的参数，但它一直优于其他模型，如FractalNet和Wide ResNets有超过30M的参数。 我们还强调，L = 100和k = 12的DenseNet-BC（例如，C10 +上的4.51％比4.62％，C100 +上的22.27％比22.71％） 与1001层预激活ResNet使用90％参数的可达到同等的性能。图4（右图）显示了这两个网络在C10 +上的训练损失和测试误差。 1001层深的ResNet收敛到较低的训练损失值，但有类似的测试错误。 我们在下面进行更详细地分析。 

<div align=center>
<img src="zh-cn/img/DenseNet/p5.png" />
</div>

**拟合能力**。更高效利用参数的一个作用是DenseNets不易发生过拟合。在不使用数据增强的数据集上，我们发现到DenseNet结构和之前的工作相比较，其改进还是很明显的。在C10上，误差降了29%，从7.33%降到了5.19%。在C100上，降了大约30%，从28.2%降到了19.64%。通过实验，我们发现一个潜在的过拟合现象：在C10上，通过将 k 从12增加到24，模型的参数量增加了4倍，而误差却从5.77%增加到5.83%。DenseNet-BC的bottleneck和compression层似乎是应对这种现象的一种有效措施。

#### 4.4 ImageNet分类结果

我们在ImageNet分类任务上测试了不同深度和增长速率的DenseNet-BC的误差，并且和ResNet结构的性能进行了比较。为了对这两种结构有一个公平的比较，我们排除了其他所有的因素，如数据预处理方式、优化器设置。我们仅仅将DenseNet-BC网络替代ResNet模型，而保留ResNet的其他实验参数不变。

我们记录了DenseNets在ImageNet上single-crop和10-crop的验证误差，如表3所示。 

<div align=center>
<img src="zh-cn/img/DenseNet/p6.png" />
</div>

DenseNets和ResNets single-crop的top-1验证误差如图3所示，其中左图以参数量为变量，右图以flops为变量。

<div align=center>
<img src="zh-cn/img/DenseNet/p7.png" />
</div>

如图3所示，与ResNets相比，在相同性能的前提下DenseNets参数量和计算量更小。例如，拥有20M参数的DenseNet-201的误差率和拥有超过40M参数的101-ResNet误差率相近。从图3的右图也可以看到类似的结果：和ResNet-50计算量接近的DenseNet大约是ResNet-101计算量的两倍。

值得注意的是，我们是修改和ResNets对应的超参数而不是DenseNets的。我们相信还可以通过修改更多的超参数来优化DenseNet在ImageNet上的性能。

### 5.讨论

从表面上看，DenseNets和ResNet非常相似：Eq.（2）与式（1）只在Hℓ输入（•）连接起来而不是加和。 然而，这个看起来很小的修改的影响导致两个网络架构的本质大不相同。 
**模型简化性（compactness）**。将输入进行连接的直接结果是，DenseNets每一层学到的特征图都可以被以后的任一层利用。该方式有助于网络特征的重复利用，也因此得到了更简化的模型。

图4左边的两张图展示了实验的结果，左图比较了所有DenseNets的参数效率，中图对DenseNts和ResNets的参数效率进行了比较。我们在C10+数据上训练了不同深度的多个小模型，并绘制出准确率。和一些流行的网络（如AlexNet、VGG）相比，pre-activation的ResNets的准确率明显高于其他网络。之后，我们将DenseNet（ k=12 ）与该网络进行了比较。DenseNet的训练集同上节。

如图4，DenseNet-BC是参数效率最高的一个DenseNet版本。此外，DenseNet-BC仅仅用了大概ResNets 1/3的参数量就获得了相近的准确率（中图）。该结果与图3的结果相一致。如图4右图，仅有0.8M参数量的DenseNet-BC和有10.2M参数的101-ResNet准确率相近。

**隐含的深度监督（implicit deep supervision）**。稠密卷积网络可以提升准确率的一个解释是，由于更短的连接，每一层都可以从损失函数中获得监督信息。可以将DenseNets理解为一种“深度监督”（deep supervision）。深度监督的好处已经在之前的深度监督网络（DSN）中说明，该网络在每一隐含层都加了分类器，迫使中间层也学习判断特征（discriminative features）。

DenseNets和深度监督网络相似：网络最后的分类器通过最多两个或三个过渡层为所有层都提供监督信息。然而，DenseNets的损失函数值和梯度不是很复杂，这是因为所有层之间共享了损失函数。

**随机vs确定连接**。稠密卷积网络与残差网络的随机深度正则化（stochastic depth
regularzation）之间有着有趣的关系。在随机深度中，残差网络随机丢掉一些层，直接将周围的层进行连接。因为池化层没有丢掉，所以该网络和DenseNet有着相似的连接模式：以一定的小概率对相同池化层之间的任意两层进行直接连接——如果中间层随机丢掉的话。尽管这两个方法在根本上是完全不一样的，但是DenseNet关于随机深度的解释会给该正则化的成功提供依据。

**特征重复利用**。根据设计来看，DenseNets允许每一层获得之前所有层（尽管一些是通过过渡层）的特征图。我们做了一个实验来判断是否训练的网络可以重复利用这个机会。我们首先在C10+数据上训练了 L=40、k=12 的DenseNet。对于每个block的每个卷积层 l  ，我们计算其与 s 层连接的平均权重。三个dense block的热度图如图5所示。

<div align=center>
<img src="zh-cn/img/DenseNet/p8.png" />
</div>

平均绝对权重用作卷积层在其前面层上的依赖性的替代物。 位置(ℓ,s)上的红点表示层ℓ，平均使用之前产生的s层的特征图。 可以从图中得到几个观察结果：

+ 1.在同一个block中，所有层都将它的权重传递给其他层作为输入。这表明早期层提取的特征可以被同一个dense block下深层所利用；
+ 2.过渡层的权重也可以传递给之前dense block的所有层，也就是说DenseNet的信息可以以很少的间接方式从第一层流向最后一层；
+ 3.第二个和第三个dense block内的所有层分配最少的权重给过渡层的输出，表明过渡层输出很多冗余特征。这和DenseNet-BC强大的结果有关系；
+ 4.尽管最后的分类器也使用通过整个dense block的权重，但似乎更关注最后的特征图，表明网络的最后也会产生一些高层次的特征。

### 6.结论

我们提出了一个新的卷积网络结构，称之为稠密卷积网络（DenseNet）。它将两个相同特征图尺寸的任意层进行连接。这样我们就可以很自然的设计上百层的网络，还不会出现优化困难的问题。在我们的实验中，随着参数量的增加，DenseNets的准确率也随之提高，而且也没有出现较差表现或过拟合的现象。通过超参数的调整，该结构在很多比赛的数据上都获得了不错的结果。此外，DenseNets有更少的参数和计算量。因为我们只是在实验中调整了对于残差网络的超参数，所以我们相信通过调整更多的超参数和学习率，DenseNets的准确率还会有更大的提升。

遵循这个简单的连接规则，DenseNets可以很自然的将自身映射（identity mappings）、深度监督（deep supervision）和深度多样化（diversified depth）结合在一起。根据我们的实验来看，该结构通过对网络特征的重复利用，可以学习到更简单、准确率更高的模型。由于简化了内部表征和降低了特征冗余，DenseNets可能是目前计算机视觉领域中在卷积网络方面非常不错的特征提取器。在以后的工作中我们计划研究DenseNets下的特征迁移工作。

### Reference

[1].[DenseNet代码实现](https://github.com/liuzhuang13/DenseNet)

[2].[DenseNet paper](https://arxiv.org/pdf/1608.06993.pdf)