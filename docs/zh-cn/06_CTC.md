
## CTC：Connectionist Temporal Classification(连接主义时间分类)

<!-- https://distill.pub/2017/ctc/

https://zhuanlan.zhihu.com/p/579232601

https://zhuanlan.zhihu.com/p/422357323 -->

<!-- https://zhuanlan.zhihu.com/p/418791107 -->

<!-- 彻底在CTC搞懂前向和后向推理，这样在其他模型就不再做解释 -->

<div align=center>
    <img src="zh-cn/img/ch6/p1.gif"   /> 
</div>


### 1.背景介绍

<!-- https://zhuanlan.zhihu.com/p/579232601 -->

**1.1 需要对齐**

考虑语音识别。 我们有一个音频剪辑数据集和相应的转录文本。 不幸的是，我们不知道，但是想知道，文本中的字符如何与音频对齐。 这使得训练语音识别器（ASR models）比乍看起来更难。

因为只有有了更细粒度的对齐，我们才可以“复用”一个字符的多种读音切片，从而更好地在基本粒度（25ms的语音切片vs. 一个个的英文字母，或者中文汉字，或者日文假名+汉字）上建模这种对应关系了。

在其他一些领域，例如OCR，图片中的哪些部分对应一个单词，也是需要**“对齐”**的。以及在机器翻译中，源语言和目标语言的哪些词word，哪些短语phrase，是可以“对齐”的。

如果没有这种对齐方式，我们将无法使用简单的方法来把长的wav和文本进行“切割”并对一个字符上可能的“独立”的发音进行建模估计。反过来也一样，一段语音，包括了哪些“独立的”发音基本单元phonemes（音素），然后每个音素又对应了具体哪些字符，这是需要建模的。

<div align=center>
    <img src="zh-cn/img/ch6/p2.png"   /> 
</div>


**1.2 对齐很难**

我们可以设计一个规则，比如“一个字符对应十个输入（帧，例如25ms算一“帧”）”。 但是人的语速是不一样的，所以这种规则总是可以被打破的。 另一种替代方法是将每个字符手动（？大坑。。。无法手动的）对齐到其在音频中的位置。 从建模的角度来看，这很有效--我们会知道每个输入时间步的基本事实。 然而，对于任何合理大小的数据集，手工搞对齐都是反人性的，是非常耗时的。

这个问题不仅仅出现在语音识别中。 我们在许多其他地方看到它。 从图像或笔划序列进行的手写识别OCR就是一个例子(如上图所示)。


**1.3 引入CTC**

连接主义时间分类 (CTC) 是一种不需要显式知道“输入”和“输出”之间的“对齐”的方法。或者说，它是可以自己使用动态规划的方法，覆盖输入和输出之间的所有可能的对齐，然后计算loss的一种方法。 正如我们将看到的，它特别适合语音和手写识别等应用。

为了更正式一点，让我们考虑映射,输入序列：
$$X = [ x_1,x_2,...,x_T ]$$

例如音频，到对应的输出序列：
$$Y=[y_1,y_2,...,y_U]$$

如转录文本。我们想要找到从 $X$ 到 $Y$ 的“准确”映射（或者说，所有“正确”的映射->在所有可行的映射的基础上，估计每个对齐的“概率分布”）。

如果尝试使用比CTC更简单的有监督学习算法的话，会遇到一些挑战。特别是：

+ $X$ 和 $Y$ 的长度都可以变化。
+ $X$ 和 $Y$ 的长度比例可以变化。
+ 我们没有 $X$ 和 $Y$ 的准确对齐（元素的对应关系，在语音文件中就是语音切片，在文本中就是一个个的字符）。
+ 
CTC 算法克服了这些挑战。对于给定的 $X$（语音），它为我们提供了所有可能的 $Y$ （文本）的输出分布。我们可以使用此分布来推断可能的输出或评估给定输出的概率。
并非所有**计算损失函数**和**执行推理**的方法都易于处理。我们将要求 CTC 有效地完成这两项工作。

+ 训练阶段：我们需要计算损失函数，从而进行错误反向传播，梯度计算，以及参数更新；
+ 推理阶段：我们需要根据训练出来的“模型”，来搜索，从而预估新输入的语音，所可能对应的输出的文本的文字。

损失函数：对于给定的输入，我们希望训练我们的模型以最大化它分配给正确答案的概率。为此，我们需要有效地计算条件概率 $p(Y∣X)$。函数 $p(Y∣X) $也应该是可微的，所以我们可以使用梯度下降优化算法。

推理：当然，在我们训练模型之后，我们想用它来推断给定 语音$X$ 之后的，可能的 文本$Y$。这意味着解决
$$Y^{\*}=argmax_{Y} p(Y|X)$$

理想情况下，我们当然希望$Y^{\*}$ 可以高效地被我们的搜索算法找到。CTC为我们提供了一个不太昂贵的近似解决方案。

### 2.CTC 算法


CTC 算法可以为给定 （本文还是主要以语音识别为例子）语音$X$的任何 文本$Y$ 来分配概率。 计算这个概率的关键是 CTC 如何考虑输入和输出之间的对齐。 我们将从查看这些对齐开始，然后展示如何使用它们来计算损失函数和执行推理。

**2.1 对齐**

CTC 算法是无需（显式的）对齐的--它不需要输入和输出之间的显式对齐。 然而为了获得给定输入的输出概率，CTC 通过对两者之间（X和Y之间）所有可能对齐的概率求和来工作。 我们需要了解这些对齐是什么，以便了解最终如何基于对齐来计算损失函数。

为了启发 CTC 对齐的特定形式，首先考虑一种幼稚的方法。 让我们举个例子。 假设输入长度为 6 且 $Y = [c, a, t]$。 对齐 $X$ 和 $Y$的一种方法是为每个输入步骤（输入帧，例如25ms的一段语音）分配一个输出字符并折叠重复.如下图所示：

<div align=center>
    <img src="zh-cn/img/ch6/p3.png"   /> 
</div> <p align=center>不引入$\epsilon$的情况下的对齐示例</p>

这种方法有两个问题:
+ 问题1：通常强制每个输入步骤都与某些输出对齐是没有意义的。 例如，在语音识别中，输入可能有一段没有相应文字输出的静音。
+ 问题2：我们无法产生连续多个字符的输出,考虑对齐 `[h, h, e, l, l, l, o]`。 折叠重复将产生`“helo”`而不是`“hello”`。

为了解决这些问题，CTC 为允许的输出集引入了一个新token。 这个新token有时被称为空白token (blank token)。 我们在这里将其称为 epsilon or ϵ。 ϵ 标记不对应任何东西（思考：那它是对应到空格吗？从下面的例子来看，它不对应到空格,更像是分隔符->分割每个相对“独立”的音素用的），只是最后的时候，epsilon会被从输出中删除。

**CTC 允许的对齐长度与输入相同。** 我们允许在合并重复并删除 ϵ 标记后映射到 $Y$ 的任何对齐：

<div align=center>
    <img src="zh-cn/img/ch6/p4.png"   /> 
</div> <p align=center>引入$\epsilon$之后的示例:合并相同的,删除$\epsilon$，然后得到结果</p>

如果 文本$Y$在一行中有两个相同的字符（例如上面的`l` 和`l`），那么有效的对齐方式必须在它们之间有一个ϵ。 有了这条规则，我们就可以区分折叠为`“hello”`的对齐方式和折叠为`“helo”`的对齐方式。

让我们回到输入长度为 6 的输出 `[c, a, t]`。 以下是更多有效（左边三个）和无效（右边三个）对齐的示例：

<div align=center>
    <img src="zh-cn/img/ch6/p5.png"   /> 
</div> <p align=center>正确的对齐（左边） vs. 错误的对齐（右边）</p>

CTC 对齐具有一些显著的特性:

+ 首先，$X$ 和 $Y$ 之间允许的对齐是单调的。 如果我们前进到下一个输入，我们可以保持相应的输出相同（即`x[t+1]`和`x[t]`对应同一个字符）或前进到下一个（字符）。
+ 第二个属性是 $X$ 到 $Y$ 的对齐是多对一的。 一个或多个输入元素可以与单个输出元素对齐，但反之则不然（即输入语音的切割粒度永远要比输出文本的切割粒度更“细”/更“稠密”）。
+ 第三，文本$Y$的长度不能大于 语音$X$ 的（切片后的切片的个数）长度。

**2.2 损失函数**

CTC对齐为我们提供了一种从“输入语音的每个时间步长的概率”走到“输出序列的每个字符的概率”的自然方式。一个示例如下图所示：

<div align=center>
    <img src="zh-cn/img/ch6/p6.png"   /> 
</div> <p align=center>beam=3的时候的beam search的示例，$\epsilon$有参与进来</p>


这个图中，输入的语音信号，一共有10个时间步，这10个时间步会扔给例如RNN等序列编码器，然后每个时间步会对应到一个分布，代表是的是当前时间步在目标文字序列的词表上每个词（or，字符）的概率。灰度越深，则表示概率越大。

然后进行beam search，即：top-3 （n-best） 输出的收集，收集之后，删除占位符号，就得到了最后的三个结果。之后每个结果会有自己的一个“概率”。这个概率，既可以用来训练模型，也可以用来执行推理时候的最优输出的搜索。

数学化表示为：

<div align=center>
    <img src="zh-cn/img/ch6/p7.png"   /> 
</div> 

使用 CTC 训练的模型，可以使用循环神经网络 (RNN)，或者Transformer ，来估计每个时间步长的概率 $p_t(a_t|X)$。 例如，RNN 通常工作得还可以，因为它考虑了输入中的上下文，但我们可以自由使用任何学习算法，这些算法在给定语音输入的固定大小切片（例如25ms为一个切片）的情况下产生输出文字的分布。

需要关注的是，计算 CTC loss可能会非常昂贵。 我们可以尝试简单的方法，暴力罗列出所有可能的对齐，并计算每个对齐的分数，并在我们进行source到target的对齐的时候，将它们全部加起来。 问题是存在大量的对齐方式。对于大多数问题，这太慢了。

幸运的是，我们可以使用动态规划算法更快地计算损失。 这里的关键点是，如果两个对齐在同一时间步骤得到相同的文字输出，那么我们可以合并它们。

<div align=center>
    <img src="zh-cn/img/ch6/p8.png"   /> 
</div> <p align=center>谁TM知道这个图在干啥</p>

上面这个图太玄幻，谁TM知道这是毛意思？细看左边这个图，好多箭头啊，我们从最右边最上面一个节点往回看，就容易理解一些了：

<div align=center>
    <img src="zh-cn/img/ch6/p9.png"   /> 
</div> 

即，红色箭头表示，只有一个完整的箭头。问题接踵而至：上面这个图中，横向表示“时间步”？纵向表示词表中的文字？ 非也，其实应该把上面的图进行一定的修改如下（即，每一列代表一个时间步，每一行代表一个文本字符）：

<div align=center>
    <img src="zh-cn/img/ch6/p10.png"   /> 
</div> <p align=center>t1, t2, t3, t4都对齐到c，（红色箭头）</p>

第一个对齐什么含义？相当于把四个时间步（语音的），都映射到了一个字符c。即，只有一个对齐，可以实现把所有的时间步，都映射到输出文本序列的第一个字符。

继续，如果把t1到t4映射到`c a`这两个字符上，那么一共有多少对齐（注意，是所有可能的对齐，这里暂时先抛开“对齐的合理性”的判断！）的方式呢？反过来想，首先，至少t4需要映射到a；其次，其他的t1, t2, t3映射到谁，是c还是a，感觉无所谓了（保证单调就好，即，不能是t1到a，而t2到c，这就出现交叉了！）


<div align=center>
    <img src="zh-cn/img/ch6/p11.png"   /> 
</div>


这四个对齐（不管是否合理），其实对照的就是t4上面的（第二行，第四列）四个黑色箭头了。同理，在第三行，第四列上，一共有7个箭头，分别看一下：

还是从后向前看：字符`"t"`可以对应到`t4`，`t3 t4`，`t2 t3 t4`，甚至`t1 t2 t3 t4`：

1.当字符`"t"`，只对应到`t4`的时候；则需要`t1, t2, t3`这三个时间戳对应到`c a`，细节呢？

`t3-a; t1/t2-c`；（即，下面会提到的，1号路：`t1-c; t2-c; t3-a; t4-t`）

`t2/t3-a; t1-c`；（2号路：`t1-c; t2-a; t3-a; t4-t`）

`t1/t2/t3-a`; （4号路：`t1-a; t2-a; t3-a; t4-t`）


<div align=center>
    <img src="zh-cn/img/ch6/p12.jpg"   /> 
</div>

2.当字符t，对应到t3和t4的时候，则需要t1, t2这两个时间戳对应到`c a`，细节：

`t2-a; t1-c`; （3号路：`t1-c; t2-a; t3-t; t4-t`）

`t1/t2-a`; （5号路：`t1-a; t2-a; t3-t; t4-t`）

<div align=center>
    <img src="zh-cn/img/ch6/p13.jpg"   /> 
</div>

3.当字符t，对应到t2, t3, t4的时候，则需要t1这一个时间戳，对应到`c a`：

`t1-c` （不能够跳？->按照图中的意思，是的，不能跳；这个路径在当前图的语境下，无效！）

`t1-a` （6号路：`t1-a; t2-t; t3-t; t4-t`）

<div align=center>
    <img src="zh-cn/img/ch6/p14.jpg"   /> 
</div>

4.当字符t，对应到t1, t2, t3, t4这四个时间戳的时候，则只有一种情况了：

`t1/t2/t3/t4-t`。（7号路：`t1-t; t2-t; t3-t; t4-t` ）


<div align=center>
    <img src="zh-cn/img/ch6/p15.jpg"   /> 
</div>

这样算下来，一共是7个有效的完整的对齐。或者反过来思考一下：


<div align=center>
    <img src="zh-cn/img/ch6/p16.jpg"   /> 
</div>

上面给出了三条路径（一种颜色的箭头组成的路径，代表一个完整的对齐）：

+ 1号路：`t1-c; t2-c; t3-a; t4-t`
+ 2号路：`t1-c; t2-a; t3-a; t4-t`
+ 3号路：`t1-c; t2-a; t3-t; t4-t`

<div align=center>
    <img src="zh-cn/img/ch6/p17.png"   /> 
</div>

+ 4号路：`t1-a; t2-a; t3-a; t4-t`
+ 5号路：`t1-a; t2-a; t3-t; t4-t`
+ 6号路：`t1-a; t2-t; t3-t; t4-t`
+ 7号路：`t1-t; t2-t; t3-t; t4-t`

需要注意的是，上面七个所谓的“对齐”，只是为了引入“动态规划”的思想，这7个对齐其实对于后面的（ASR）语境来说，是有问题的！！因为有些文字，没有被对齐到时间步！这是不允许的！！

显然这个问题，适用于动态规划方法来做。

<div align=center>
    <img src="zh-cn/img/ch6/p18.jpg"   /> 
</div>


这个图什么含义呢？

例如，对于t1来说，它可以分别对齐到`c, a`, 和`t`；（左上角第一个灰色的圈，代表的是t1对齐到c；第二行最左边一个灰色的圈，代表t1对齐到a）

对于t2来说，它可以对齐到和t1相连的文字，也可以选择对齐到“和t1相连的文字的下一个”；

依此类推，每个时间步，都可以有两个选择：

+ 选择1，和前一个时间步相同（如果当前是第一个时间步，那它可以选择输出文字序列中的任意一个位置）；
+ 选择2，是前一个时间步连接的文字的下一个文字（那如果没有下一个文字了怎么办？那就对齐到一个占位符$\epsilon$了-虽然这并不合理）。

因为这种“占位符”可以出现在每个字符的前面和后面，那么我们就可以扩展原来的输出文本序列为：

$$z=[\epsilon,y_1,\epsilon,y_2,...,\epsilon,y_U,\epsilon]$$

**0.干货来了**

我们用$\alpha$来表示给定节点上合并对齐的分数。 更准确地说，**$\alpha_{s,t}$是序列$z_{1:s}$在输入语音的t时间步骤之后的CTC得分。**


正如我们将看到的，我们将根据最后一个时间步的α ，来计算最终的 CTC 分数 $p(Y∣X)$【这个也是动态规划的所谓“出口”】。根据动态规划的思想，只要我们知道前一时间步的α 的值，我们就可以计算 $α_{s, t}$。有两种情况：

**1.情况1: 如果$z_{s-1}\neq \epsilon$或者$z_s=z_{s-2}$**

<div align=center>
    <img src="zh-cn/img/ch6/p19.png"   /> 
</div>

注意这种情况下$\alpha_{s,t}$只能从$\alpha_{s,t-1}$或者$\alpha_{s-1,t-1}$中转移，原因在于$z_{s-1}\neq\epsilon$时$z_{s-1}必须有对齐，而如果$z_s=z_{s-2}$则$z_{s-1}的$\epsilon$必须有对齐，因为我们需要分隔重复字符以保证最终输出的正确性。用图表示：

<div align=center>
    <img src="zh-cn/img/ch6/p21.jpg"   /> 
</div>

**2.情况2：除此之外的情况下我们可以从$\alpha_{s-2,t-1}$进行转移，因此有：**

<div align=center>
    <img src="zh-cn/img/ch6/p20.png"   /> 
</div>

用图表示为：

<div align=center>
    <img src="zh-cn/img/ch6/p22.jpg"   /> 
</div>


以下是动态规划算法执行的计算示例。 每个有效对齐在此图中都有一个路径和其相对应。


<div align=center>
    <img src="zh-cn/img/ch6/p23.png"   /> 
</div> <p align=center>$\alpha_{s=4, t=4}$，属于”情况2“，不属于”情况1“</p>


前面论述过，有两种“情况”，

+ 绿色两个箭头表示要考虑的是$\alpha_{s-1, t-1}$和$\alpha_{s, t-1}$；
+ 黄色的三个箭头，表示要考虑的是$\alpha_{s-2, t-1}$，$\alpha_{s-1, t-1}$和$\alpha_{s, t-1}$这三个已经通过动态规划算法计算出来的东西。

鉴于当前`t=4, s=4`（编号都是从1开始），是`a ϵ b`的形式，所以“情况1”不适用，我们只能使用“情况2”的动态规划公式。从而发现了从$\alpha_{s-2, t-1}$，$\alpha_{s-1, t-1}$和$\alpha_{s, t-1}到$\alpha_{s,t}$的图上的三个“黑色”的箭头。


### 3.CTC训练

到目前为止我们已经描述了一种允许RNN用于CTC的输出表示，现在推导出一个用梯度下降训练CTC网络的目标函数。目标函数是根据最大似然原理推导出来的。也就是说，最小化它会最大化目标标签的对数似然。

请注意，这与标准神经网络目标函数的原理相同（Bishop，1995）。 给定目标函数及其相对于网络输出的导数，权重梯度可以通过标准反向传播随时间计算。 然后可以使用当前用于神经网络的任何基于梯度的优化算法来训练网络（LeCun 等人，1998 年；Schraudolph，2002 年）。 我们从最大似然函数所需的算法开始。

**3.1 CTC前向-后向算法**

我们需要一种计算单个“标签序列”的条件概率 $p(l|x)$ 的有效方法,这里：
$$p(l|x)=\sum_{\pi \in \beta^{-1}(l)}p(\pi|x)$$

关于符号的一些说明：
+ $l$:输出文字序列;
+ $x$:输入的语音信号序列;
+ $\beta^{-1}(l)$:代表的是一个集合，即所有可以产生出$l$这个目标文字序列的path的集合;
鉴于有多个路径path，都可以让$x$对应到一个具体的$l$序列，所以我们需要把每个path上的概率都加起来，就是$p(l|x)$了。


乍一看上述公式这将是有问题的：“总和”是在与给定“标签序列”相对应的所有路径（对齐）上的，并且通常有很多这样的路径。幸运的是，这个问题可以用**动态规划算法**解决，类似于 HMM 的前向后向算法（Rabiner，1989）。

关键思想是，**对应于标签的路径的“总和”可以被分解为，对应于该标签前缀的路径的迭代总和。**然后可以使用递归的前向和后向变量有效地计算迭代。


对于某个长度为$r$的序列$q$，分别用$q[1:p]$和 $q[r-p:r]$ 表示它的前$p$个字符和最后$p$个字符。然后对于标记序列$l$，将前向变量（forward variable） $\alpha_t(s)$定义为时间步$t$处$l[1:s]$（即该序列的前$s$个字符） 的总概率。

注意：$\alpha$的下标$t$，是源语音中的frame的序号；括号里面的$s$，是目标文字序列中的label的序号！


<div align=center>
    <img src="zh-cn/img/ch6/p26.jpg"   /> 
</div>

上述公式的符号解释：

+ $y$是概率！$y$的上标$t^{'}$是语音frame的标号;$y$的下标是一个具体的$label=\pi_{t^{'}}$;
+ $\pi \in N^{T}$并且没有写成 $\pi \in N^{t}$，是有一定的原因的！即我们只考虑那些”完整的，合法的“paths。如果是只到$N^t$，那么可能后续因为对齐的问题，导致这个path后续作废的可能。所以，我们只考虑$N^T$这个在完整的序列上是合法的的path的集合.

正如我们将看到的，$\alpha_t(s)$可以从$\alpha_{t-1}(s)$和$\alpha_{t-1}(s-1)$递归计算。为了允许在输出路径中出现空白$\epsilon$，

我们考虑修改后的标签序列 $l^{'}$，在开头和结尾添加空白，并插入每对标签之间。例如原来是`ab`，那么改造之后，是 `ϵ  a ϵ  b ϵ `了,即开头和结尾都有$\epsilon$，而且`a`和`b`之间也有$\epsilon$。

因此 $l^{'}$ 的长度是 $2|l| + 1$. 在计算 $l^{'}$的前缀概率时，我们允许空白和非空白标签之间的所有转换，以及任何一对不同的非空白标签之间的转换。我们允许所有前缀以空格 `(b)` 或 $l$ 中的第一个符号(即：$l_1$)开头,这为我们提供了以下初始化规则：

<div align=center>
    <img src="zh-cn/img/ch6/p27.jpg"   /> 
</div>

这里在强调一遍：
+ $\alpha$是前向概率;
+ $\alpha$的下标1，表示的是 source voice frame的标号;
+ $\alpha$的括号里面的`1，2，s`，表示的是target textual sequence的具体的token/label的位置标号;
+ $y$是概率;
+ $y$的上标1，表示的是source voice frame的标号；
+ $y$的下标，表示的是一个具体的target sequence上的一个label。

<div align=center>
    <img src="zh-cn/img/ch6/p28.jpg"   /> 
</div>

注意
$$\alpha_t(s)=0, \forall s < |l^{'}|-2(T-t)-1$$


因为这些变量对应于，这样一些状态，即：没有足够时间步长来完成序列（下图 右上角的未连接的圆圈-红色三角区域）。

<div align=center>
    <img src="zh-cn/img/ch6/p29.png"   /> 
</div>

同样也有：
$$\alpha_t(s)=0,\forall s<1$$

那么标签序列 $l$ 的概率是 $l^{'}$ 的概率之总和，（$l^{'}$包括了，在时间$T$上有和没有最后的空白）.

$$p(l|x)=\alpha_T(|l^{'}|)+\alpha_T(|l^{'}|-1)$$

同样，后向变量（backward variables）$\beta_t(s)$  定义为时间步$t$上， $l[s:|l|]$ 处的总概率:


<div align=center>
    <img src="zh-cn/img/ch6/p30.jpg"   /> 
</div>

其实，理解了前向传播算法的精髓之后，后向传播的动态规划公式就不难理解了。

注意:$\beta_t(s)=0,\forall s>2t$(下图 左下角的未连接圆-绿色三角区域）和$\beta_t(s)=0, \forall s>|l^{'}|$

<div align=center>
    <img src="zh-cn/img/ch6/p31.png"   /> 
</div>

在实践中，上述递归将很快导致大多数数字计算机上的下溢。避免这种情况的一种方法是重新调整前向和后向变量（Rabiner，1989）的刻度（即，进行缩放操作）。如果我们定义

<div align=center>
    <img src="zh-cn/img/ch6/p32.png"   /> 
</div>

并在（6）和（7）的 RHS（right-hand-side，等式右边） 上用$\hat{\alpha}$来替代$\alpha$，则前向变量将被保持在可计算范围内。

类似地，对于后向变量，我们定义 ：

<div align=center>
    <img src="zh-cn/img/ch6/p33.png"   /> 
</div>

并替换 (10) 和 (11) 的 RHS 上，用 $\hat{\beta}$ 来替代$\beta$ 。

为了评估最大似然误差，我们需要目标标签序列的概率的自然对数。使用重新调整完毕刻度（缩放之后）的变量，它们具有特别简单的形式：

<div align=center>
    <img src="zh-cn/img/ch6/p34.png"   /> 
</div>


**3.2 最大似然训练**

最大似然训练的目的是，同时最大化“训练集`”S={(x, z)}`中所有（模型输出中，被判定为正确的分类的）正确分类的对数概率。在我们的例子中，这意味着最小化以下目标函数：

<div align=center>
    <img src="zh-cn/img/ch6/p35.png"   /> 
</div>

+ 上面的$O$表示，objective，目标函数；
+ $ML$表示机器学习，machine learning；
+ `S={(x,z)}`，训练集样本集合；
+ $N_w$就是前面定义的一个“连续映射-函数”（或者，就是一个神经网络，实现了一个映射函数），即：

<div align=center>
    <img src="zh-cn/img/ch6/p36.png"   /> 
</div>

为了使用梯度下降算法来训练网络，我们需要关于公式（12）对于“神经网络输出”$y=N_w(x)$的微分。

由于训练样本之间是独立的，我们可以分别考虑它们，下面只看一个样本$(x, z)$的目标函数，对于对第$t$时间步（而且候选是文字$k$的）的概率（ $y_k^t$ : 它被解释为，在时间步$t$上观察到文字$k$的概率，这里的文字$k$，指的是文字在词典上的编号）的偏微分：

<div align=center>
    <img src="zh-cn/img/ch6/p37.png"   /> 
</div>


我们现在展示，如何用3.1 节的“前向-后向”算法，来计算公式（13）

关键在于，对于一个标签序列$l$，给定$s$（当前关注“字符”） 和$t$ 处（当前关注时间步）的前向变量$\alpha_t(s)$  和后向变量 $\beta_t(s)$ 的乘积是与 $l$ 对应的所有路径（=所有对齐）在时间$t$通过符号$s$的概率。

更准确地说，从公式（5）和公式（9）我们有：

<div align=center>
    <img src="zh-cn/img/ch6/p38.png"   /> 
</div>

<div align=center>
    <img src="zh-cn/img/ch6/p39.png"   /> 
</div>

<div align=center>
    <img src="zh-cn/img/ch6/p40.png"   /> 
</div>

解释一下吧。其实如果对PCFG，或者HMM比较熟悉的话，那边的forward-backward，以及inside-outside算法和这里的是非常类似的。主要看这个乘积是怎么来的。

$\pi$：【一个对齐】是可以最终产出出来“标签序列`l`”的一个可行的完整的对齐路径（所谓“完整”，指的是，每个时间步，从`t=1`到`T`，都确定对应到了标签序列`l`中的一个字符。）;

$\pi_t=l_{s}^{'}$这个表示的是，在这个对齐（即可以最终产出出来“标签序列`l`”的一个可行的完整的对齐路径）中，对于时间步`s`，唯一对应到的字符就是标签序列$l^{'}$上的`s`。而且$l^{'} = l$的加$\epsilon$的处理化：例如`l=a b`，那么`l'=ϵ a ϵ b ϵ`。

<div align=center>
    <img src="zh-cn/img/ch6/p41.png"   /> 
</div>


所以，这里的$\sum$  下面的下标是没有问题的。然后就是三个$\Pi$  的乘积了，这个的确不容易理解。而且仔细理解，需要重点理解下图绿色方框的部分（因为后面会有一个微分，把这个绿色的方框干掉）！

<div align=center>
    <img src="zh-cn/img/ch6/p42.png"   /> 
</div>

直观理解上，再说一遍上面的forward和backward的乘积：
+ 其一可以构造出来标签序列`l`的对齐有很多，我们都需要；
+ 其二在这些对齐中，从`(s,t)`点上，向左向右看，我们只关注的是满足$\pi_t=l^{'}$  的那些对齐！

再看看之前的公式:
<div align=center>
    <img src="zh-cn/img/ch6/p43.png"   /> 
</div>

重新排列上面的 $\alpha_t(s)\beta_t(s)$，并且才考上式中关于$p(\pi|x)$  【这也正是，给定输入$x$的前提条件下，一个完整的对齐的概率】的定义，我们可以得到：

<div align=center>
    <img src="zh-cn/img/ch6/p44.png"   /> 
</div>

从下面公式我们可以看到这是总概率 $p(l|x)$ 的一部分，因为这些路径在时间 $t$ 穿过 $l_s^{'}$

<div align=center>
    <img src="zh-cn/img/ch6/p45.png"   /> 
</div>

对于任何$t$，我们因此可以对所有 $s$ 求和得到：

<div align=center>
    <img src="zh-cn/img/ch6/p46.png"   /> 
</div>

需要注意的是，这里仍然使用的是辅助标签文字序列$l^{'}$，而不是最初的标签序列$l$。为了对上面的公式基于其中的$y_k^{t}$微分，我们只需要考虑那些在时间$t$ 穿过标签$k$的路径。

注意到对于单个标签序列$l$，相同的标签（或空白）可能会重复多次【类似于`- a a a a-`，这种，`a`】，我们将标签 `k` 出现的位置集定义为： 
$$lab(l,k)= \{[s: l_s^{'}=k ]\}$$
这个集合可能是空的。

<div align=center>
    <img src="zh-cn/img/ch6/p47.png"   /> 
</div>

观察到：

<div align=center>
    <img src="zh-cn/img/ch6/p48.png"   /> 
</div>


<div align=center>
    <img src="zh-cn/img/ch6/p50.png"   /> 
</div>

最后，为了通过`softmax`层反向传播梯度，我们需要相对于非归一化输出 $u^t_k$ 的目标函数导数。如果使用3.1节的重新缩放，我们有：

<div align=center>
    <img src="zh-cn/img/ch6/p49.png"   /> 
</div>

这个类似于：`y = softmax(u)` 然后，`dO/du = dO/dy * dy/du`,前面的`dO/dy`，就是公式(13)了；而后边的`dy/du`就是softmax函数的微分。

### 4.CTC推断

在我们训练好模型之后，我们想用它来寻找给定输入的可能输出。 更准确地说，我们需要解决：

<div align=center>
    <img src="zh-cn/img/ch6/p51.png"   /> 
</div>

一种启发式方法是，在每个时间步中采用最可能的输出。 这给了我们拥有最高概率的一个对齐：

<div align=center>
    <img src="zh-cn/img/ch6/p52.png"   /> 
</div>



**4.1 beam search**

然后我们可以折叠重复（去除重复的字符）并删除 ϵ 标记以获得 Y.

对于许多应用程序，这种启发式搜索（解码）方法效果很好，尤其是当大部分概率质量分配给单个对齐时。然而，这种方法有时会以更高的概率错过容易找到的输出。问题是，它没有考虑到单个输出可以有多个对齐的事实。（或者说，多个可能的对齐，最终都是映射到一个文字序列的输出）

一个例子。假设对齐 `[a, a, ϵ]` 和 `[a, a, a]`分别具有比 `[b, b, b]` 低的概率。但它们的概率之和实际上大于 `[b, b, b]`的概率之和。朴素的启发式将错误地提出 `Y= [b]` 作为最可能的假设。但是，它应该选择 `Y = [a]`。

为了解决这个问题，算法需要考虑到 `[a, a, a]` 和 `[a, a,ϵ]`折叠为相同输出`[a]`的事实。

我们可以使用修改后的beam搜索来解决这个问题。鉴于有限的计算，修改后的beam搜索不一定会找到最可能的 Y(即beam搜索不保证一定能搜索到全局最优解)。它至少有一个很好的特性，我们可以为渐近更好的解决方案权衡更多的计算（更大的beam size）。

常规beam搜索在每个输入步骤（=输入时间步）计算一组新假设（new hypotheses）。通过使用所有可能的输出字符扩展每个假设(each hypothesis)并仅保留顶级候选者，从前一组生成新的假设集(the new set of hypotheses)。

<div align=center>
    <img src="zh-cn/img/ch6/p53.png"   /> 
</div> <p align=center>beam size=3的时候的beam search示例。注意，这个示例，原封不动地对每个T下的prefix按照词典集合扩展。（没有绘制可能的“merge”等操作）。</p>

上图给出了`beam size=3`的时候的beam search的例子，这里的目标词表大小为3，只包括`{ϵ, a, b}`。

**4.2 beam合并**

我们可以修改普通beam搜索，来处理映射到同一输出的多个对齐。 在这种情况下，我们不是在beam中保留对齐列表，而是在折叠重复并删除 ϵ 字符后，保存输出前缀。 在搜索的每一步，我们都会根据映射到它（=输出前缀）的所有对齐来累积给定前缀的分数。

<div align=center>
    <img src="zh-cn/img/ch6/p54.png"   /> 
</div> <p align=center>beam search保存：合并后的“输出结果前缀”</p>

可以看到，这个时候，前缀的表示，有了变化了，保存的其实是到目前为止，经过”折叠“，去除ϵ之后的纯正的”前缀序列“。这样，就可以根据这个”前缀序列“来执行”搜索路径的合并“操作了。

例如`T=3`的时候的前缀`a`，其实是来自三个路径的，`T=2`的时候的`λ a`，`a a`，以及`a ϵ`。这三个路径，都会最终产出`a`这个前缀。

如果字符是重复字符，则建议的扩展可以映射到两个输出前缀（为啥？原因看下面的例子：）。 这在上图中的 `T=3` 处显示，其中“a”被提议作为前缀 `[a]` 的扩展。

`T=3`的时候的`a`，可以是来自`T=2`的时候的，
+ `λ a`或者`a a` 
+ `a ϵ`
这两个完全不一样:
+ `λ a`，如果在`T=3`的时候，再扩展出一个`a`，就是`λ a a` -> `a`
+ `a ϵ`，如果在`T=3`的时候，再扩展出一个`a`，就是`a ϵ a` -> `a a`

当我们扩展 `[a]` 以产生 `[a,a]` 时，我们只希望包含之前以`ϵ` 结尾的对齐分数的部分（什么含义？->也就是说只有是类似`a ϵ` + `a`的形式，我们才有可能扩展出`a a`）。 需要再次提醒的是，重复字符之间需要 `ϵ`分割！ 也就是类似下图的示意：

<div align=center>
    <img src="zh-cn/img/ch6/p55.png"   /> 
</div> <p align=center>如果是T=4的时候，前缀为a a ，那么可以推断：T=3的时候，一定是a ϵ 过来的！</p>

按照上图的黄色箭头的示意，如果是`T=4`的时候，前缀为`a a` ，那么`T=3`的时候，一定是`a ϵ` 过来的！

类似地，当我们目前的前缀为 `[a]` 时，我们应该只包含前一个分数中不以 `ϵ` 结尾的对齐部分,如下图所示：

<div align=center>
    <img src="zh-cn/img/ch6/p56.png"   /> 
</div> <p align=center>如果是T=4的时候，前缀为a a ，那么可以推断：T=3的时候，一定是a ϵ 过来的！</p>

鉴于此，我们必须跟踪beam中每个前缀的两个概率：
+ 以 ϵ 结尾的所有对齐的概率
+ 不以ϵ 结尾的所有对齐的概率

当我们在修剪beam之前对每一步的假设（hypothesis）进行排名时，我们将使用它们的组合分数（就是这两个概率相加，而不是beam扩展的时候的相乘！注意，如果是在对数概率维度，那么`p1 + p2 -> exp(log(p1)) + exp(log(p2))`）。


<div align=center>
    <img src="zh-cn/img/ch6/p57.png"   /> 
</div> <p align=center>多个“扩展”可以合并到一个“Hypothesis”；一个“扩展”也可以分割成两个“hypotheses”</p>

上图中说的是这三点：

+ 其一：多个“扩展”可以合并到一个“Hypothesis”；例如上图的`λ a`;`a ϵ`； `a a `这三个扩展，对应到了一个hypothesis，其prefix为`a`；
+ 其二：一个“扩展”也可以分割成两个“hypotheses”；例如上图的`a`被扩展的时候，需要再次区分`λ a`；`a a`这两个扩展（因为它们对应到a），和`a ϵ`这一个扩展，因为它对应到`a a`。
+ 其三：故此即使是三个扩展都到`a`，那还是需要区分开，`a` 和`a ϵ`的

具体可以直接参考代码实现！


**4.3 加入语言模型**

在某些问题中，例如语音识别，在输出上加入语言模型可以显着提高准确性。 我们可以将语言模型作为推理问题的一个因子。

<div align=center>
    <img src="zh-cn/img/ch6/p58.png"   /> 
</div> 

函数 $L(Y)$根据语言模型标记计算 $Y$ 的长度，并作为单词插入奖励。$Y$序列和语言模型的“粒度”需要统一：

+ 使用基于单词word的语言模型 $L(Y)$ 计算 $Y$中的单词word数。
+ 如果我们使用基于character的语言模型，则 $L(Y)$ 计算$Y$ 中的character数。


注意，语言模型分数仅在：beam search 前缀由字符（或单词）扩展(即，计算【“目前为止给定前缀”+“新字符”】的语言模型分数）时才包括在内，而不是在算法的每个步骤中。 这导致搜索偏爱较短的前缀(所以用$L(Y)$来一致算法生成短文字)。 参数 α 和 β 通常通过交叉验证设置。


### 5.CTC性质

到目前为止，我们提到了 CTC 的一些重要属性。 在这里，我们将更深入地了解这些属性是什么以及它们提供了什么权衡。

**5.1 条件独立性**

CTC 最常被引用的缺点之一是它做出的条件独立假设。

<div align=center>
    <img src="zh-cn/img/ch6/p24.jpg"   /> 
</div> <p align=center>CTC 的图形模型，可以看到a1和a2...aT都是相互条件独立的，即给定X的条件下，他们相互独立</p>

该模型假设每个输出在给定输入的情况下都与其他输出有条件独立。 对于许多序列到序列问题，这是一个糟糕的假设。


假设我们有一个人说`“triple A”`的音频。除了按照发音直接输出`“triple A”`之外，另一个有效的转录可能是`“AAA”`。 如果预测转录的第一个字母是`“A”`，那么下一个字母应该是`“A”`（高概率）和(其他任何不同于A的字母的话，例如)`“r”`（低概率）。 条件独立假设不允许这样做【从而这是有问题的】。


<div align=center>
    <img src="zh-cn/img/ch6/p25.png"   /> 
</div> <p align=center>如果第一个字母预测的是A，则后缀AA应该有更高的概率；如果首字母预测是t，则riple应该有更高的概率！</p>

上图也示例了，当wav发音是"triple A"的时候，ASR的解码过程中：

+ 如果第一个字母预测的是`A`，则后缀`AA`应该有更高的概率；
+ 如果首字母预测是`t`，则`riple`应该有更高的概率！

事实上，使用 CTC 的语音识别器不会像条件依赖的模型那样学习输出的语言模型。但是，可以包含单独的语言模型(即，单独训练一个新的语言模型，做post-ranking)，并且通常可以很好地提高准确性.

CTC 做出的条件独立假设并不总是坏事。 对输出之间的“交互”进行强假设下的建模的话，会使模型不太适应新的领域或进化后的领域。 例如，我们可能想要使用“经过朋友之间电话交谈训练”（领域1）的语音识别器，来转录客户支持电话（领域2）。 即使声学模型相似，这两个领域中的语言也可能完全不同。 使用 CTC 声学模型，我们可以在更改领域时轻松切换新的语言模型.


**5.2 对齐属性**

CTC 算法是alignment-free的。目标函数在所有对齐上做边缘化margin求和。虽然 CTC 确实对 X 和 Y 之间的对齐形式做出了强有力的假设，但该模型不知道概率如何在它们之间分布。在某些问题中，CTC 最终将大部分概率分配给单个对齐（top-1的对齐占据超过了50%的概率的一种情况）。但是，这并不能被100%保证。

如前所述，CTC 只允许单调对齐。在诸如语音识别之类的问题中，这可能是一个有效的假设。对于其他问题，例如机器翻译，其中目标句子中的未来词可以与源句子的较早部分对齐，这种假设是不成立的（在机器翻译领域）。

CTC 对齐的另一个重要特性是它们是多对一的。多个输入最多可以与一个输出对齐。在某些情况下，这可能是不可取的。我们可能希望在 X 和 Y 的元素之间强制执行严格的一一对应。或者，我们可能希望允许多个输出元素与单个输入元素对齐。例如，字符“th”可能与音频的单个输入步骤（单个时间切片的语音）对齐。但是，基于字符的 CTC 模型不允许这样做。

多对一属性意味着输出的时间步长不能多于输入。 这对于语音和手写识别通常不是问题，因为输入比输出长得多。但是，对于 Y 通常比 X 长的其他问题，CTC 就行不通了。

### 6.基于CTC的流式语音识别模型的训练与测试

+ https://github.com/ASR-studio/CTC_ASR