## RNN-T(Sequence Transduction with Recurrent Neral Networks)

<!-- https://blog.csdn.net/qq_50749521/article/details/126147321 -->

<!-- https://www.cnblogs.com/mollnn/p/15423897.html -->
<!-- https://bat.sjtu.edu.cn/zh/rnn-t/ -->


### 1.前言

基于联结时序分类(CTC)的声学模型不再需要对训练的音频序列和文本序列进行强制对齐，实际上已经初步具备了端到端的声学模型建模能力。但是CTC模型进行声学建模存在着三个严重的瓶颈，一是缺乏语言模型建模能力，不能整合语言模型进行联合优化，二是不能建模模型输出之间的依赖关系，三是同一个音频特征输入无法输出多个状态预测。RNN-Transducer针对CTC的不足，进行了改进，使得模型具有了端到端联合优化、具有语言建模能力、便于实现Online语音识别等突出的优点, 更加适合语音任务。

### 2.RNN-T介绍

RNN-Transducer模型实际上是在CTC模型的一种改进。因此，本文从CTC模型出发，一步步引入为什么要使用RNN-T对语音识别任务建模，RNN-T模型还有什么问题存在。

在联结时序分类模型（CTC）提出之前，深度神经网络-隐马尔可夫模型占据着语音识别的江山。但是其需要预先对数据进行强制对齐，以提供给模型逐帧标记，用于监督训练。CTC巧妙对给定输入序列，所有可能输出路径的条件概率进行建模，实现了不需要强制对齐就能进行序列与序列之间的转换。由于CTC要对所有可能的路径求概率值，如果枚举所有路径的话会有很高的计算代价，因此将动态规划的思路引入CTC路径的概率计算过程，这就是前后向算法。


CTC对于语音识别的声学建模带来了极大的好处

+ 化繁为简，不在需要强制对齐，可以使用文本序列本身来进行学习训练
+ 加速解码，大量Blank的存在，使得模型在解码过程中可以使用跳帧操作，因此大大加速了解码过程。

但是CTC模型仍然存在着很多的问题，其中最显著的就是CTC假设模型的输出之间是条件独立的。这个基本假设与语音识别任务之前存在着一定程度的背离。此外，CTC模型并不具有语言建模能力，同时也并没有真正的实现端到端的联合优化。CTC对齐的一个重要特性是它们是多对一的。多个输入最多可以与一个输出对齐。在某些情况下，这可能是不可取的。我们可能希望在 X 和 Y 的元素之间强制执行严格的一一对应。或者，我们可能希望允许多个输出元素与单个输入元素对齐。例如，字符“th”可能与音频的单个输入步骤（单个时间切片的语音）对齐。但是，基于字符的 CTC 模型不允许这样做。

针对CTC的不足，Alex Graves在2012年左右提出了RNN-T模型，RNN-T模型巧妙的将语言模型声学模型整合在一起，同时进行联合优化，是一种理论上相对完美的模型结构。RNN-T模型引入了TranscriptionNet也就是图中的Encoder（可以使用任何声学模型的结构），相当于声学模型部分，图中的PredictionNet实际上相当于语言模型（可以使用单向的循环神经网络来构建）。模型中比较新奇，同时也是最重要的结构就是联合网络Joint Net，一般可以使用前向网络来进行建模。联合网络的作用就是将语言模型和声学模型的状态通过某种思路结合在一起，可以是拼接操作，也可以是直接相加等，考虑到语言模型和声学模型可能有不同的权重问题，似乎拼接操作更加合理一些。

<div align=center>
    <img src="zh-cn/img/ch8/p1.png"   /> 
</div>


我们细化上述结构图：RNN-T 网络在RNA网络的基础上使每个输入vector可以连续输出多个token，当每个token输出符号为Φ时，RNN-T网络再开始接受下一个frame的vector，具体过程如下图所示：

<div align=center>
    <img src="zh-cn/img/ch8/p2.png"   /> 
</div>

其实，在RNN-T中，RNN网络的的输出并不是简单的将上一时刻的输出作为当然时刻的一个输入，而是将上一时刻的输出放入一个额外的RNN中，然后将额外RNN的输出作为当前时刻的一个输入；这个额外的RNN可以认为是一个语言模型（PredictionNet)，可以单独在语料库上进行训练，因为在一般的语料库上并不包含$\Phi$符号，因此这个额外的RNN网络在训练时会忽略符号$\Phi$

<div align=center>
    <img src="zh-cn/img/ch8/p4.png"   /> 
</div>


<div align=center>
    <img src="zh-cn/img/ch8/p3.png"   /> 
</div>

这样完美的解决了CTC的3个弊端。

像CTC一样，模型需要生成一张解码图，在解码图上使用前后向算法对所有可能的路径进行概率计算，其解码图如下所示。

<div align=center>
    <img src="zh-cn/img/ch8/p5.png"   /> 
</div>

解码图中所有横向转移表示根据第$t$个声学模型状态和第$u$个预测出的标记，解码出空转移的概率，垂直方向的转移均为非空转移。


### 3.RNN-T的优缺点

RNN-T模型本身是比较完善的，几乎汇集了所有模型的优点
+ 端到端联合优化
+ 具有语言模型建模能力
+ 具有单调性，能够进行实时在线解码。
如果非得挑RNN-T模型的不足的话无非两点：
+ 模型比较难以训练

针对上述问题，比较好的解决思路就是进行预训练，其中Google尝试了一种CTC多级预训练机制，模型对声学模型和语言模型分别进行预训练，其中声学模型预训练模型使用CTC作为损失函数，在越底层使用的建模单元越小，在越高层，使用的建模单元越大，然后使用预训练模型与RNN-T声学模型部分同级的权重去做预训练，虽然这个一个繁琐的工作，但是也不失为一种很好的预训练方法。在Google的这项工作中，在grapheme作为建模单元基础上，引入了词组单元wordpieces，能够捕获更长的文本信息，有利于减少替换性错误。对于其中的CTC部分，采用多级CTC，建模单元包括音素phoneme、字母grapheme、词条wordpieces，此外在字母LSTM输出时，通过时域卷积来缩短时间片长度，减少参数量，加速训练。

<div align=center>
    <img src="zh-cn/img/ch8/p6.png"   /> 
</div>


### 4.Training and Testing

+ 训练阶段：给定输入声学特征序列$x$，输出目标序列$y$。训练的目标就是为了最大化输出概率$P(y|x)$, 也就是最小化损失函数$−lnP(y|x)$。可以使用前向-后向优化算法进行梯度计算和后向传播。
+ 测试阶段：给定$x$，解码目标就是找到概率最大的目标序列$y^{*}$:

$$y^{*}=argmax_{y}P(y|x)=argmax_{y}\sum_{\hat{y}}P(\hat{y}|x)$$

如果罗列出所有可能路径，再一一转成目标序列，比较概率大小，是非常麻烦的。
如果用`prefix-search decoding`解码方式，也就是每时刻找到输出概率最大的那个，最后得到目标序列。
但这样一条最大概率路径并不一定是最优解，这是因为，我们最终的目标序列可能会有很多条路径概率相加，这多条路径的概率加起来会比最大概率的那一条路径概率更大。
所以我们一般会用beam search找出n-best。简单来说，$t_1$时刻，找出$P(k|1,0)$的三个最大值，以此为节点，$t_2$时刻继续向下扩展，每次都找出3-best，一直走到最后。


